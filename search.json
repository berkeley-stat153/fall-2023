[
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "License"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week 1: Aug 23 - Aug 25\nCharacteristics of time series\npdf, html, source\n\n\n\nWeek 2: Aug 28 - Sep 1\nMeasures of dependence\npdf, html, source\n\n\n\nWeek 3: Sep 6 - Sep 8\nRegression and prediction\npdf, html, source\nHw 1 due Thur Sep 7\n\n\nWeek 4: Sep 11 - Sep 15\nRegression and prediction\n”\n\n\n\nWeek 5: Sep 18 - Sep 22\nRegularization and smoothing\npdf, html, source\nHw 2 due Thur Sep 21\n\n\nWeek 6: Sep 25 - Sep 29\nRegularization and smoothing\n”\n\n\n\nWeek 7: Oct 2 - Oct 6\nSpectral analysis\npdf, html, source\nHw 3 due Fri Oct 6\n\n\nWeek 8: Oct 9 - Oct 13\nSpectral analysis\n”\n\n\n\nWeek 9: Oct 16 - Oct 20\nARIMA models\npdf, html, source\nMidterm Fri Oct 20\n\n\nWeek 10: Oct 23 - Oct 27\nARIMA models\n”\n\n\n\nWeek 11: Oct 30 - Nov 3\nARIMA models\n”\n\n\n\nWeek 12: Nov 6 - Nov 8\nETS models\npdf, html, source\nHw 4 due Mon Nov 6\n\n\nWeek 13: Nov 13 - Nov 17\nETS models\n”\n\n\n\nWeek 14: Nov 20\n[Nothing! Enjoy Thanksgiving]\n\n\n\n\nWeek 15: Nov 27 - Dec 1\nAdvanced topics\npdf, html, source\nHw 5 due Fri Dec 1, Final Weds Dec 13"
  },
  {
    "objectID": "lectures/smoothing/smoothing.html",
    "href": "lectures/smoothing/smoothing.html",
    "title": "Lecture 4: Regularization and Smoothing",
    "section": "",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(astsa)\nlibrary(fpp3)\nlibrary(epidatasets)\n\n\n\nLagged regression\n\ny = as.numeric(cmort) # Response vector\nlags = 4 * 0:10       # Lags 0, 4, 8, 12, ... 40\n\n# Build predictor matrix, and run linear regression\nx = matrix(NA, length(y), length(lags))  \nfor (j in 1:length(lags)) {\n  x[,j] = dplyr::lag(as.numeric(part), lags[j])\n}\n\nreg = lm(y ~ x)\ncoef(reg)\n\n (Intercept)           x1           x2           x3           x4           x5 \n54.840747976  0.218774708  0.146177521  0.146738025  0.105053522  0.038851794 \n          x6           x7           x8           x9          x10          x11 \n 0.004758640  0.046766034  0.043647238  0.005335550 -0.034166678 -0.006548196 \n\n\n\n\nRidge\n\nlibrary(glmnet) # Implements ridge and lasso estimators\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\n# We'll need to omit NA values from x (and y, correspondingly) explicitly, as\n# otherwise glmnet will complain\nna_obs = 1:max(lags)\nx = x[-na_obs, ]\ny = y[-na_obs]\n\n# Ridge regression: set alpha = 0, lambda sequence will be chosen automatically\nridge = glmnet(x, y, alpha = 0)\nbeta_ridge = coef(ridge)\nlambda_ridge = ridge$lambda\n\ndim(beta_ridge)      # One row per coefficient, one column per lambda value\n\n[1]  12 100\n\nbeta_ridge[1:3, 1:5] # E.g., first 3 coefficients for first 5 lambda values\n\n3 x 5 sparse Matrix of class \"dgCMatrix\"\n                      s0           s1           s2           s3           s4\n(Intercept) 8.849359e+01 8.845060e+01 8.844641e+01 8.844182e+01 8.843678e+01\nV1          3.052009e-37 6.119754e-04 6.712496e-04 7.362242e-04 8.074384e-04\nV2          3.614721e-37 7.241482e-04 7.942171e-04 8.710100e-04 9.551601e-04\n\nmatplot(log(lambda_ridge), t(beta_ridge[-1, ]), type = \"l\", col = 1:8, \n        xlab = \"log(lambda)\", ylab = \"Coefficient estimate\", main = \"Ridge\")\nlegend(\"topright\", legend = paste(\"Lag\", lags), lty = 1:5, col = 1:8)\n\n\n\n\n\n\n\n\n\n\nLasso\n\n# Lasso regression: set alpha = 1, and let glmnet() choose a lambda sequence \n# itself, automatically\nlasso = glmnet(x, y, alpha = 1)\nbeta_lasso = coef(lasso)\nlambda_lasso = lasso$lambda\n\ndim(beta_lasso)      # One row per coefficient, one column per lambda value\n\n[1] 12 66\n\nbeta_lasso[1:3, 1:5] # E.g., first 3 coefficients for first 5 lambda values\n\n3 x 5 sparse Matrix of class \"dgCMatrix\"\n                  s0          s1          s2          s3          s4\n(Intercept) 88.49359 86.64912678 84.88663639 83.28028242 81.81704580\nV1           .        .           .           .           .         \nV2           .        0.01618761  0.03490427  0.05197942  0.06751764\n\nmatplot(lambda_lasso, t(beta_lasso[-1, ]), type = \"l\", col = 1:8, \n        xlab = \"lambda\", ylab = \"Coefficient estimate\", main = \"Lasso\")\nlegend(\"topright\", legend = paste(\"Lag\", lags), lty = 1:5, col = 1:8)\n\n\n\n\n\n\n\n\n\n\nMoving average\n\nwgts = rep(1, 11) / 11\nma = stats::filter(soi, sides = 2, filter = wgts)\nplot(soi, col = 8, ylab = \"Southern Oscillation Index\", \n     main = \"Moving average\")\nlines(ma, lwd = 2, col = 4)\npar(fig = c(0.55, 1, 0.55, 1), new = TRUE) # The top-right insert\nnwgts = c(rep(0, 20), wgts, rep(0, 20))\nplot(nwgts, type = \"l\", ylim = c(-0.02, 0.1), \n     xaxt = \"n\", yaxt = \"n\", ann = FALSE)\n\n\n\n\n\n\n\n\n\n\nKernel smooth\n\nks = ksmooth(time(soi), soi, kernel = \"normal\", bandwidth = 1)\nplot(soi, col = 8, ylab = \"Southern Oscillation Index\", \n     main = \"Kernel smoother\")\nlines(ks, lwd = 2, col = 4)\npar(fig = c(0.55, 1, 0.55, 1), new = TRUE) # The top-right insert\ncurve(dnorm(x), type = \"l\", xlim = c(-3, 3), ylim = c(-0.02, 0.45),  \n     xaxt = \"n\", yaxt = \"n\", ann = FALSE)\n\n\n\n\n\n\n\n\n\n\nHP filter\n\nboston = boston_marathon |&gt;\n  filter(Year &gt;= 1924) |&gt;\n  filter(Event == \"Men's open division\") |&gt;\n  mutate(Minutes = as.numeric(Time)/60) |&gt;\n  select(Year, Minutes)\n\nn = nrow(boston)\nD = diag(rep(-2,n))       # -2s on the diagonal\nD[row(D) == col(D)-1] = 1 # 1s above the diagonal\nD[row(D) == col(D)+1] = 1 # 1s below the diagonal\nD = D[-c(1,n), ]          # Drop first and last row\nD[1:6, 1:6]               # Take a quick look\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1   -2    1    0    0    0\n[2,]    0    1   -2    1    0    0\n[3,]    0    0    1   -2    1    0\n[4,]    0    0    0    1   -2    1\n[5,]    0    0    0    0    1   -2\n[6,]    0    0    0    0    0    1\n\nI = diag(n)               # n x n identity matrix\n\n# Compute and plot HP filter solution at a few lambda values\nlam = c(10, 100, 1000)\nhp = matrix(NA, nrow = n, ncol = length(lam))\nfor (i in 1:length(lam)) {\n  hp[ ,i] = solve(I + lam[i] * t(D) %*% D, boston$Minutes)\n}\n\nplot(boston$Year, boston$Minutes, col = 8, \n     xlab = \"Year\", ylab = \"Time\", main = \"Hodrick-Prescott filter\")\nmatplot(boston$Year, hp, type = \"l\", lty = 1, lwd = 2, col = 2:4, add = TRUE)\nlegend(\"topright\", paste(\"lambda =\", lam), lty = 1, lwd = 2, col = 2:4)\n\n\n\n\n\n\n\n\n\n\nTrend filter\n\nlibrary(glmgen) # Implements trend filtering\n\n# Compute and plot trend filter solution at a few lambda values: the lambda\n# sequence will be chosen automatically (as in glmnet)\ntf = trendfilter(boston$Minutes, k = 1)\n\nind = c(20, 10, 1)\nplot(boston$Year, boston$Minutes, col = 8, \n     xlab = \"Year\", ylab = \"Time\", main = \"Trend filter\")\nmatplot(boston$Year, tf$beta[, ind], type = \"l\", \n        lty = 1, lwd = 2, col = 2:4, add = TRUE)\n# knots = which(abs(D %*% tf$beta[, ind[2]]) &gt; 1e-5) + 1\n# abline(v = boston$Year[knots], lty = 2, col = 3)\nlegend(\"topright\", paste(\"lambda =\", round(tf$lambda[ind])), \n       lty = 1, lwd = 2, col = 2:4)"
  },
  {
    "objectID": "lectures/regression/regression.html",
    "href": "lectures/regression/regression.html",
    "title": "Lecture 3: Linear Regression and Prediction",
    "section": "",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(astsa)\nlibrary(fpp3)\nlibrary(epidatasets)\n\n\n\nChicken regression\n\nhead(chicken)\n\n[1] 65.58 66.48 65.70 64.33 63.23 62.94\n\nhead(time(chicken))\n\n[1] 2001.583 2001.667 2001.750 2001.833 2001.917 2002.000\n\nreg = lm(chicken ~ time(chicken))\ncoef(reg)\n\n  (Intercept) time(chicken) \n -7131.022465      3.592109 \n\nplot(chicken, ylab = \"Chicken price (cents per pound)\") \nabline(coef(reg), col = 2)\n\n\n\n\n\n\n\n\n\n\nCardiovascular mortality regression\n\n# Simple regression\npar(mfrow = c(2, 1), mar = c(2, 4, 0.75, 0.5))\nplot(cmort, xlab = \"\", ylab = \"Cardiovascular deaths\") \nplot(part, xlab = \"\", ylab = \"Particulate levels\")\n\n\n\n\n\n\n\nreg_part = lm(cmort ~ part)\ncoef(reg_part)\n\n(Intercept)        part \n 74.7985953   0.2931732 \n\npar(mfrow = c(1, 1), mar = c(4, 4, 0.5, 0.5))\nplot(part, cmort, xlab = \"Particulate levels\", ylab = \"Cardiovascular deaths\")\nabline(coef(reg_part), col = 2, lwd = 2)\n\n\n\n\n\n\n\n\n\n\nCardiovascular mortality, multiple\n\n# Multiple regression\npar(mfrow = c(3, 1), mar = c(2, 4, 0.75, 0.5))\nplot(cmort, xlab = \"\", ylab = \"Cardiovascular deaths\") \nplot(part, xlab = \"\", ylab = \"Particulate levels\")\nplot(tempr, xlab = \"\", ylab = \"Temperature\")\n\n\n\n\n\n\n\nreg_tempr = lm(cmort ~ tempr)\ncoef(reg_tempr)\n\n(Intercept)       tempr \n124.8324611  -0.4865793 \n\nreg_joint = lm(cmort ~ part + tempr)\ncoef(reg_joint)\n\n(Intercept)        part       tempr \n110.5453149   0.2882667  -0.4782371 \n\npar(mfrow = c(1, 1), mar = c(4.5, 4.5, 0.5, 2))\nplot(part, cmort, xlab = \"Particulate levels\", ylab = \"Cardiovascular deaths\")\nabline(coef(reg_part), col = 2, lwd = 2)\nabline(coef(reg_joint)[c(1,2)], col = 4, lty = 2, lwd = 2)\n\n\n\n\n\n\n\nplot(tempr, cmort, xlab = \"Temperature\", ylab = \"Cardiovascular deaths\")\nabline(coef(reg_tempr), col = 2, lwd = 2)\nabline(coef(reg_joint)[c(1,3)], col = 4 , lty = 2, lwd = 2)\n\n\n\n\n\n\n\npar(mar = c(4.5, 4.5, 0.5, 0.5))\nacf(residuals(reg_part), ylab = \"Auto-correlation\")\n\n\n\n\n\n\n\npacf(residuals(reg_part), ylab = \"Partial auto-correlation\")\n\n\n\n\n\n\n\n\n\n\nLagged features, wrong way\n\n# Regression with lagged features\nk = 4 # Let's consider a 4 week lag\nplot(part, xlab = \"\", ylab = \"Particulate levels\")\nlines(stats::lag(part, k), col = 2)\n\n\n\n\n\n\n\n# BEWARE!!! The WRONG WAY to do it!!!\ncoef(lm(cmort ~ stats::lag(part, k)))\n\n        (Intercept) stats::lag(part, k) \n         74.7985953           0.2931732 \n\n# Why is this wrong? Because it is the same as un-lagged regression\ncoef(lm(cmort ~ part))\n\n(Intercept)        part \n 74.7985953   0.2931732 \n\ncoef(lm(cmort ~ stats::lag(part, 1)))\n\n        (Intercept) stats::lag(part, 1) \n         74.7985953           0.2931732 \n\ncoef(lm(cmort ~ stats::lag(part, 2)))\n\n        (Intercept) stats::lag(part, 2) \n         74.7985953           0.2931732 \n\ncoef(lm(cmort ~ stats::lag(part, 3)))\n\n        (Intercept) stats::lag(part, 3) \n         74.7985953           0.2931732 \n\n# Why does this happen? Because stats::lag(x, k) returns a vector of the same \n# length as x, and just shifts the time axis back k time steps, but lm() doesn't \n# know about the time axis, and it just regresses one vector onto another ...\nhead(part)\n\n[1] 72.72 49.60 55.68 55.16 66.02 44.01\n\nhead(stats::lag(part, k))\n\n[1] 72.72 49.60 55.68 55.16 66.02 44.01\n\nrange(time(part))\n\n[1] 1970.00 1979.75\n\nrange(time(stats::lag(part, k)))\n\n[1] 1969.923 1979.673\n\n\n\n\nLagged features, right way\n\n# The solution is to convert these to vectors and use dplyr::lag()\ncmort_vec = as.numeric(cmort)\npart_vec = as.numeric(part)\n\n# Now look at this! As expected, the first 4 are NA because we don't have lag 4 \n# values there\nx = dplyr::lag(part_vec, k)\nhead(part_vec)\n\n[1] 72.72 49.60 55.68 55.16 66.02 44.01\n\nhead(x)\n\n[1]    NA    NA    NA    NA 72.72 49.60\n\n# Since lm() omits NA values by default, we are fine to do the regression. The \n# fitted model is different (different coefficients), as expected\nreg_lagged = lm(cmort_vec ~ x)\ncoef(reg_lagged)\n\n(Intercept)           x \n 72.3477118   0.3438611 \n\n# With this model we can make k (true, out-of-sample) forecasts, since we have\n# not used the last k particular level measurements in fitting the regression\nyhat = predict(reg_lagged, newdata = data.frame(x = tail(part_vec, k)))\n\n# Note: predict() can be VERY ANNOYING as requires you to create a data frame\n# whose variable is the same new as the covariate used in the call to lm() ...\n# Important to be aware of this (which is why we saved the lagged feature as a\n# simple variable called x), and it will save you lots of pain debugging later\n\n# We can plot these predictions, but we can't validate them (we don't have data \n# on what happened after 1970)\npar(mar = c(2, 4, 2, 0.5))\ndelta = 1 / frequency(cmort)\nplot(cmort, xlim = c(min(time(cmort)), max(time(cmort)) + k * delta),    \n     xlab = \"\", ylab = \"Cardiovascular deaths\", main = \"Prospective forecasts\")\nlines(max(time(cmort)) + (1:k) * delta, yhat, col = 2)\n\n\n\n\n\n\n\n\n\n\nSplit-sample forecasts\n\n# To validate them, we can refit the regression on the first half of the time\n# series, make forecasts on the second half, and compare to what was observed\nn = length(cmort)\nfirst_half = 1:n &lt;= floor(n/2)\nsecond_half = 1:n &gt; floor(n/2)\nreg_lagged_first_half = lm(cmort_vec ~ x, subset = first_half)\ncoef(reg_lagged_first_half)\n\n(Intercept)           x \n 74.4085159   0.3844513 \n\nyhat_second_half = predict(reg_lagged_first_half, \n                           newdata = data.frame(x = x[second_half]))\nmae_second_half = mean(abs(cmort[second_half] - yhat_second_half))\n\npar(mar = c(2, 4, 2, 0.5))\nplot(time(cmort)[first_half], cmort[first_half], type = \"l\", \n     xlim = range(time(cmort)), ylim = range(cmort),\n     xlab = \"\", ylab = \"Cardiovascular deaths\", \n     main = \"Split-sample forecasts\")\nlines(time(cmort)[second_half], cmort[second_half], col = 8, lty = 2)\nlines(time(cmort)[second_half], yhat_second_half, col = 2)\nlegend(\"topright\", legend = c(\"Observed\", \"Predicted\"), \n       lty = c(2, 1), col = c(8, 2))\nlegend(\"topleft\", legend = paste(\"MAE =\", round(mae_second_half, 2)), \n       bty = \"n\")\n\n\n\n\n\n\n\n\n\n\nCross-validation\n\n# Now refit the regression sequentially on the second half, as we walk forward\n# in time: this is called time series cross-validation. We'll actually do this\n# with two models: one that fits on all past, and one that fits on the last 10\n# time points\nt0 = floor(n/2)\nyhat_all_past = yhat_trailing = rep(NA, length = n-t0)\nw = 10 # This is our trailing window length\n\nfor (t in (t0+1):n) {\n  reg_all_past = lm(cmort_vec ~ x, subset = (1:n) &lt;= t-k)\n  reg_trailing = lm(cmort_vec ~ x, subset = (1:n) &lt;= t-k & (1:n) &gt; t-k-w)\n  yhat_all_past[t-t0] = predict(reg_all_past, newdata = data.frame(x = x[t]))\n  yhat_trailing[t-t0] = predict(reg_trailing, newdata = data.frame(x = x[t]))\n}\n\nmae_all_past = mean(abs(cmort[second_half] - yhat_all_past))\nmae_trailing = mean(abs(cmort[second_half] - yhat_trailing))\n\npar(mar = c(2, 4, 2, 0.5))\nplot(time(cmort)[first_half], cmort[first_half], type = \"l\", \n     xlim = range(time(cmort)), ylim = range(cmort),\n     xlab = \"\", ylab = \"Cardiovascular deaths\",\n     main = \"Cross-validation, training on all past\")\nlines(time(cmort)[second_half], cmort[second_half], col = 8, lty = 2)\nlines(time(cmort)[second_half], yhat_all_past, col = 3)\nlegend(\"topright\", legend = c(\"Observed\", \"Predicted\"), \n       lty = c(2, 1), col = c(8, 3))\nlegend(\"topleft\", legend = paste(\"MAE =\", round(mae_all_past, 2)), \n       bty = \"n\")\n\n\n\n\n\n\n\nplot(time(cmort)[first_half], cmort[first_half], type = \"l\", \n     xlim = range(time(cmort)), ylim = range(cmort),\n     xlab = \"\", ylab = \"Cardiovascular deaths\", \n     main = \"Cross-validation, training on trailing window\")\nlines(time(cmort)[second_half], cmort[second_half], col = 8, lty = 2)\nlines(time(cmort)[second_half], yhat_trailing, col = 4)\nlegend(\"topright\", legend = c(\"Observed\", \"Predicted\"), \n       lty = c(2, 1), col = c(8, 4))\nlegend(\"topleft\", legend = paste(\"MAE =\", round(mae_trailing, 2)), \n       bty = \"n\")"
  },
  {
    "objectID": "lectures/arima/arima.html",
    "href": "lectures/arima/arima.html",
    "title": "Lecture 6: Autoregressive Integrated Moving Average Models",
    "section": "",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(astsa)\nlibrary(fpp3)\nlibrary(epidatasets)\n\n\n\nAR(1) processes\n\nx1 = arima.sim(list(order = c(1,0,0), ar = 0.9), n = 100)\nx2 = arima.sim(list(order = c(1,0,0), ar = -0.9), n = 100)\n\npar(mfrow = c(2, 1), mar = c(2, 0.5, 3, 0.5))\nplot(x1, main = paste(\"AR(1), phi = +0.9\"), xlab = \"\", ylab = \"\")\nplot(x2, main = paste(\"AR(1), phi = -0.9\"), xlab = \"\", ylab = \"\")\n\n\n\n\n\n\n\n\n\n\nMA(1) processes\n\nx1 = arima.sim(list(order = c(0,0,1), ma = 0.9), n = 100)\nx2 = arima.sim(list(order = c(0,0,1), ma = -0.9), n = 100)\n\npar(mfrow = c(2, 1), mar = c(2, 0.5, 3, 0.5))\nplot(x1, main = paste(\"MA(1), phi = +0.9\"), xlab = \"\", ylab = \"\")\nplot(x2, main = paste(\"MA(1), phi = -0.9\"), xlab = \"\", ylab = \"\")\n\n\n\n\n\n\n\n\n\n\nParameter redundancy\n\nx = rnorm(100)\narima(x, order = c(1,0,1), include.mean = FALSE)\n\n\nCall:\narima(x = x, order = c(1, 0, 1), include.mean = FALSE)\n\nCoefficients:\n         ar1      ma1\n      0.4012  -0.3569\ns.e.  0.6550   0.6636\n\nsigma^2 estimated as 0.8733:  log likelihood = -135.12,  aic = 276.24\n\n\n\n\nAuto-correlation\n\nx1 = arima.sim(list(order = c(2,0,0), ar = c(1.5, -0.75)), n = 500)\nx2 = arima.sim(list(order = c(0,0,3), ma = c(0.9, 0.85, 0.8)), n = 500)\n\npar(mfrow = c(2, 2), mar = c(4.5, 4.5, 0.5, 0.5))\nxlim = c(1, 25)\nylim = c(-1, 1)\ncex = 1.25\nacf(x1, xlim = xlim, ylim = ylim)\nlegend(\"topright\", legend = \"AR(2)\", bty = \"n\", cex = cex) \nacf(x2, xlim = xlim, ylim = ylim)\nlegend(\"topright\", legend = \"MA(3)\", bty = \"n\", cex = cex) \npacf(x1, xlim = xlim, ylim = ylim, ylab = \"PACF\") \nlegend(\"topright\", legend = \"AR(2)\", bty = \"n\", cex = cex) \npacf(x2, xlim = xlim, ylim = ylim, ylab = \"PACF\")\nlegend(\"topright\", legend = \"MA(3)\", bty = \"n\", cex = cex) \n\n\n\n\n\n\n\n\n\n\nAuto ARIMA?\n\nset.seed(666)\nx = rnorm(1000)         \n\n# Older forecast package\nforecast::auto.arima(x)  \n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\nSeries: x \nARIMA(2,0,1) with zero mean \n\nCoefficients:\n          ar1      ar2     ma1\n      -0.9744  -0.0477  0.9509\ns.e.   0.0429   0.0321  0.0294\n\nsigma^2 = 0.9657:  log likelihood = -1400\nAIC=2808.01   AICc=2808.05   BIC=2827.64\n\n# Newer fable package, same result (input must be a tsibble)\ndat = tsibble(data.frame(x, Time = 1:length(x)), index = Time)\ndat |&gt; model(arima = ARIMA(x ~ pdq())) |&gt; report()\n\nSeries: x \nModel: ARIMA(2,0,1) \n\nCoefficients:\n          ar1      ar2     ma1\n      -0.9744  -0.0477  0.9509\ns.e.   0.0429   0.0321  0.0294\n\nsigma^2 estimated as 0.9657:  log likelihood=-1400\nAIC=2808.01   AICc=2808.05   BIC=2827.64\n\n# Uh oh! This is basically a redundant parametrization of white noise\n\n\n\nARIMA for CAR exports\n\ncar = global_economy |&gt; filter(Code == \"CAF\")\n\n# Small bit of exploratory analysis\ncar |&gt;\n  ggplot(aes(x = Year, y = Exports)) + \n  geom_line() + geom_point() +\n  labs(title = \"Central African Republic (CAR) exports\",\n       y = \"Exports (% of GDP)\") + theme_bw() \n\n\n\n\n\n\n\ncar |&gt; \n  ggplot(aes(x = Year, y = difference(Exports))) + \n  geom_line() + geom_point() + theme_bw() \n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\npar(mar = c(4.5, 4.5, 0.5, 0.5))\nxlim = c(1, 15)\nylim = c(-0.5, 0.5)\nacf(diff(car$Exports), xlim = xlim, ylim = ylim)\n\n\n\n\n\n\n\npacf(diff(car$Exports), xlim = xlim, ylim = ylim, ylab = \"PACF\")\n\n\n\n\n\n\n\n# Now go and fit ARIMA models\ncar_fit = car |&gt;\n  model(arima210 = ARIMA(Exports ~ pdq(2,1,0)),\n        arima013 = ARIMA(Exports ~ pdq(0,1,3)),\n        auto = ARIMA(Exports))\n\ncar_fit |&gt; pivot_longer(!Country, names_to = \"Model name\",\n                         values_to = \"Orders\")\n\n# A mable: 3 x 3\n# Key:     Country, Model name [3]\n  Country                  `Model name`         Orders\n  &lt;fct&gt;                    &lt;chr&gt;               &lt;model&gt;\n1 Central African Republic arima210     &lt;ARIMA(2,1,0)&gt;\n2 Central African Republic arima013     &lt;ARIMA(0,1,3)&gt;\n3 Central African Republic auto         &lt;ARIMA(2,1,2)&gt;\n\n# Glance at various metrics across the three models\ncar_fit |&gt; glance() |&gt; select(.model:BIC) |&gt; arrange(AICc) \n\n# A tibble: 3 × 6\n  .model   sigma2 log_lik   AIC  AICc   BIC\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 arima210   6.71   -134.  275.  275.  281.\n2 arima013   6.54   -133.  274.  275.  282.\n3 auto       6.42   -132.  274.  275.  284.\n\n# Print coefficients, plot roots for the AR model\ncar_fit |&gt; select(arima210) |&gt; coef()\n\n# A tibble: 2 × 6\n  .model   term  estimate std.error statistic  p.value\n  &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 arima210 ar1     -0.505     0.127     -3.99 0.000191\n2 arima210 ar2     -0.290     0.125     -2.31 0.0245  \n\ncar_fit |&gt; select(arima210) |&gt; gg_arma()\n\n\n\n\n\n\n\n# Plot residuals from the AR model\ncar_fit |&gt; select(arima210) |&gt; gg_tsresiduals() \n\n\n\n\n\n\n\n# Make 5-year horizon forecasts from the AR model\ncar_fit |&gt;  \n  select(Country, arima210) |&gt;\n  forecast(h = 5) |&gt; \n  autoplot(car) + labs(title = \"ARIMA(2,1,0)\") + theme_bw()\n\n\n\n\n\n\n\n# Make 5-year horizon forecasts from a random walk\ncar |&gt;\n  model(rw = ARIMA(Exports ~ pdq(0,1,0) + 0)) |&gt;\n  forecast(h = 5) |&gt; \n  autoplot(car) + labs(title = \"Random walk\") + theme_bw()\n\n\n\n\n\n\n\n\n\n\nGeneral warning …\n\n# WARNING!!! The ARIMA() function may actually still do some automatic model \n# selection outside of the specified orders p,d,q. This can be dangerous because\n# it may decide to do something that you didn't realize! It will decide whether\n# to include a constant c or seasonal orders P,D,Q based on the data. To shut \n# this off, you have to FULLY specify the model\n\n# Here's an example:\noh_no = car |&gt;\n  model(model1 = ARIMA(Exports ~ pdq(2,0,0)),\n        model2 = ARIMA(Exports ~ 0 + pdq(2,0,0) + PDQ(0,0,0)))\n\n# Thought you clearly specified as AR(2) model in the first line, right? WRONG. \n# Take a look. You'll see it decided to include a constant ...\noh_no |&gt; select(model1) |&gt; report()\n\nSeries: Exports \nModel: ARIMA(2,0,0) w/ mean \n\nCoefficients:\n         ar1     ar2  constant\n      0.5725  0.3517    1.5012\ns.e.  0.1217  0.1239    0.2761\n\nsigma^2 estimated as 7.281:  log likelihood=-139.22\nAIC=286.44   AICc=287.19   BIC=294.68\n\n# The second specification is the one that always returns the AR(2) model\noh_no |&gt; select(model2) |&gt; report()\n\nSeries: Exports \nModel: ARIMA(2,0,0) \n\nCoefficients:\n         ar1     ar2\n      0.6059  0.3876\ns.e.  0.1212  0.1217\n\nsigma^2 estimated as 7.327:  log likelihood=-141.13\nAIC=288.25   AICc=288.7   BIC=294.43\n\n\n\n\nSARIMA for US employment\n\nleisure = us_employment |&gt;\n  filter(Title == \"Leisure and Hospitality\", year(Month) &gt; 2000) |&gt;\n  mutate(Employed = Employed/1000) |&gt;\n  select(Month, Employed)\n\n# Small bit of exploratory analysis\nleisure |&gt;\n  ggplot(aes(x = Month, y = Employed)) + \n  geom_line() + geom_point() +\n  labs(title = \"US employment: leisure and hospitality\",\n       y = \"Employed (millions of people)\") + theme_bw() \n\n\n\n\n\n\n\nleisure |&gt; \n  ggplot(aes(x = Month, y = difference(Employed, lag = 12))) + \n  geom_line() + geom_point() + theme_bw() \n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nleisure |&gt; \n  ggplot(aes(x = Month, y = difference(difference(Employed, lag = 12)))) + \n  geom_line() + geom_point() + theme_bw() \n\nWarning: Removed 13 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\nWarning: Removed 13 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\npar(mar = c(4.5, 4.5, 0.5, 0.5))\nxlim = c(1, 25)\nylim = c(-0.3, 0.2)\nacf(diff(diff(leisure$Employed, lag = 12)), xlim = xlim, ylim = ylim)\n\n\n\n\n\n\n\npacf(diff(diff(leisure$Employed, lag = 12)), xlim = xlim, ylim = ylim, \n     ylab = \"PACF\")\n\n\n\n\n\n\n\n# Now go and fit SARIMA models\nleisure_fit = leisure |&gt;\n  model(arima210110 = ARIMA(Employed ~ pdq(2,1,0) + PDQ(1,1,0, period = 12)),\n        arima012011 = ARIMA(Employed ~ pdq(0,1,2) + PDQ(0,1,1, period = 12)),\n        auto = ARIMA(Employed))\n\nleisure_fit |&gt; pivot_longer(everything(), names_to = \"Model name\",\n                            values_to = \"Orders\")\n\n# A mable: 3 x 2\n# Key:     Model name [3]\n  `Model name`                    Orders\n  &lt;chr&gt;                          &lt;model&gt;\n1 arima210110  &lt;ARIMA(2,1,0)(1,1,0)[12]&gt;\n2 arima012011  &lt;ARIMA(0,1,2)(0,1,1)[12]&gt;\n3 auto         &lt;ARIMA(1,1,1)(2,1,1)[12]&gt;\n\n# Glance at various metrics across the three models\nleisure_fit |&gt; glance() |&gt; select(.model:BIC) |&gt; arrange(AICc) \n\n# A tibble: 3 × 6\n  .model       sigma2 log_lik   AIC  AICc   BIC\n  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 auto        0.00143    395. -777. -777. -757.\n2 arima012011 0.00146    391. -775. -775. -761.\n3 arima210110 0.00154    387. -766. -765. -752.\n\n# Print coefficients, plot roots for the AR model\nleisure_fit |&gt; select(arima210110) |&gt; coef()\n\n# A tibble: 3 × 6\n  .model      term  estimate std.error statistic   p.value\n  &lt;chr&gt;       &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 arima210110 ar1      0.200    0.0693      2.88 0.00438  \n2 arima210110 ar2      0.201    0.0681      2.95 0.00350  \n3 arima210110 sar1    -0.308    0.0712     -4.33 0.0000235\n\nleisure_fit |&gt; select(arima210110) |&gt; gg_arma()\n\n\n\n\n\n\n\n# Plot residuals from the AR model\nleisure_fit |&gt; select(arima210110) |&gt; gg_tsresiduals() \n\n\n\n\n\n\n\n# Make 3-year horizon forecasts from the AR model\nleisure_fit |&gt;  \n  select(arima210110) |&gt;\n  forecast(h = 36) |&gt; \n  autoplot(leisure) + labs(title = \"ARIMA(2,1,0)(1,1,0)[12]\") + theme_bw()"
  },
  {
    "objectID": "supplementary.html",
    "href": "supplementary.html",
    "title": "Supplementary Resources",
    "section": "",
    "text": "We will (roughly) follow some chapters of the following two books, which you can look at as supplements to the lecture notes. The first should be available to you by searching for it online through the UC Berkeley Library, and the second is freely available at the link below.\n\nShumway, Stoffer, Time Series Analysis and Its Applications, 2017.\nHyndman, Athanasopoulos, Forecasting: Principles and Practice, 2021.\n\nBelow are two other references on time series that may be helpful as well. The first is more advanced, and the second more elementary.\n\nBrockwell, David, Introduction to Time Series and Forecasting, 2016.\nCryer, Chan, Time Series Analysis With Applications in R, 2008."
  },
  {
    "objectID": "homeworks/homework5/homework5.html",
    "href": "homeworks/homework5/homework5.html",
    "title": "Homework 5: [YOUR NAME HERE]",
    "section": "",
    "text": "The total number of points possible for this homework is 46. The number of points for each question is written below, and questions marked as “bonus” are optional. Submit the knitted html file from this Rmd to Gradescope.\nIf you collaborated with anybody for this homework, put their names here:\n\nFew notes about this homework\nFirst, a few notes about this homework. This last homework is more open-ended than the previous homeworks and is something in between a regular homework and a project. You should do your best to make all of your work and your reasoning clear, well-written, and expository in nature. In the more open-ended parts, this will help us to identify rigorous thinking and assign marks accordingly.\nThis homework is an exercise in COVID-19 forecasting. You should know that, in true (prospective) COVID-19 forecasting, the situation is much harder than the one you are facing in this homework. This is because of data revisions: the forecasters in true (prospective) COVID-19 forecasting did not have access to the same data in real-time that you have access to now, in retrospect. Instead, they had access to preliminary data that was subject to revisions, sometimes very large and irregular ones, making forecasting much harder. You can see, for example, McDonald et al. (2021), for a discussion of this.\n\n\nCOVID-19 cases and deaths\nFirst, download the files cases_deaths.csv and forecasts.csv from the course GitHub repo. The former contains weekly reported COVID-19 cases and death counts for every U.S. state, from July 4, 2020 to March 4, 2023. The latter contains 1-4 week ahead forecasts of weekly reported COVID-19 death counts for every U.S. state over that same time period. These forecasts were generated by the COVIDhub ensemble model, which is an ensemble of all qualifying forecast submissions to the U.S. COVID-19 Forecast Hub, and was the basis of official CDC communications during the pandemic. You can see Ray et al. (2022) for discussion of the ensemble model.\nThe code below loads in these data frames (which you can run once the downloaded files are in your working directory) and plots death curves along with one set of 1-4 week ahead forecasts (and 80% prediction intervals) for the each of CA, NY, and PA.\n\nlibrary(tidyverse)\n\ncases_deaths = read.csv(\"cases_deaths.csv\")\nforecasts = read.csv(\"forecasts.csv\")\ncases_deaths = cases_deaths |&gt; mutate(date = as.Date(date))\nforecasts = forecasts |&gt; mutate(target_date = as.Date(target_date))\n\nfc_date = as.Date(\"2021-08-21\")\nstates = c(\"ca\", \"ny\", \"pa\")\n\nggplot() +\n  geom_line(\n    data = cases_deaths |&gt; filter(geo_value %in% states) |&gt;\n      mutate(alpha = ifelse(date &lt;= fc_date-7, 1, 0.8)), # account for plotting\n    # quirk here: line color for a segment assigned to the point on the left\n    aes(x = date, y = deaths, alpha = alpha)) + \n  geom_vline(xintercept = fc_date, linetype = 2) +\n  geom_ribbon(\n    data = forecasts |&gt; filter(geo_value %in% states) |&gt;\n      mutate(forecast_date = target_date - ahead * 7) |&gt;\n      filter(forecast_date == fc_date), \n    aes(x = target_date, ymin = forecast_0.1, ymax = forecast_0.9,\n        fill = geo_value), alpha = 0.5) +\n  geom_line(\n    data = forecasts |&gt; filter(geo_value %in% states) |&gt;\n      mutate(forecast_date = target_date - ahead * 7) |&gt;\n      filter(forecast_date == fc_date), \n    aes(x = target_date, y = forecast_0.5, color = geo_value), \n    linewidth = 1.25) +\n  facet_wrap(vars(geo_value), scales = \"free_y\", ncol = 1) +\n  labs(x = \"Date\", y = \"Reported Covid-19 deaths\") + \n  theme_bw() + theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nAR and ARX models\n\n(4 pts) For each state, compute the sample auto-correlation function for COVID-19 deaths over the time period July 04, 2020 through March 27, 2021. This period captures the “winter 2020 wave”. Average the auto-correlation (take an average per lag) across all of the states. Plot the state-averaged auto-correlation function.\nDo the same, but for the sample cross-correlation function between COVID-19 deaths and COVID-19 cases. Plot the state-averaged cross-correlation function. Discuss what you see in these plots.\n\n\n# CODE GOES HERE\n\n\n(8 pts) For each state, separately, fit an AR model to directly forecast the death count at a horizon \\(h\\) from the latest three values: [ {t+h|t} = {k=0}^2 {h,k} , y{t-k} ] Do this for horizons \\(h=1,2,3,4\\), i.e., 1-4 weeks ahead. Note that you will thus fit 4 separate models per state.\nUse time series cross-validation (CV) to evaluate forecasts from these AR models starting at a target date of April 3, 2021 and going through the end of the data set. You can do this by manually forming the lags yourself, fitting the AR model withlm(), and then manually implementing the time series CV loop, as we did in the regression lecture (weeks 3-4: “Regression and prediction”).\nFor each horizon (1-4 weeks ahead), report the mean absolute scaled error (MASE) from time series CV, averaged over all states. Plot the state-averaged MASE from the AR models as a function of horizon.\n\n\n# CODE GOES HERE\n\n\n(4 pts) Repeat the same as in Q2, but now use an ARX model to forecast deaths at horizon \\(h\\) from the latest three values, and also the latest three case counts (this is the the exogenous, or “X” part): [ {t+h|t} = {k=0}^2 {h,k} , y{t-k} + {k=0}^2 {h,k} , x_{t-k}. ] The rest should be the same: in the end, plot the state-averaged MASE from the ARX models as a function of horizon. Compare this result to what you saw in Q2.\n\n\n# CODE GOES HERE\n\n\n(2 pts) Surprised to see the forecasts from the ARX models perform worse? Plot the (say) 1-week ahead forecasts for California from the AR and the ARX model, overlaid on top of the reported death counts, and use this to derive a plausible explanation for why the ARX model performs worse.\n\n\n# CODE GOES HERE\n\n\n(4 pts) Refit the models AR and ARX models from Q2 and Q3 so that they each use trailing training windows of 12 weeks. Compare their state-averaged MASE curves relative to what you saw in Q2 and Q3.\n\n\n# CODE GOES HERE\n\n\n(Bonus) Disaggregate the MASE by major pandemic waves: the summer 2021 wave, Delta wave, and Omicron wave. You can choose the time boundaries by eye by looking at national data (sum of all state deaths). Thus you should get three separate MASE comparisons between the 4 models.\n\n\n# CODE GOES HERE\n\n\n\nARIMA and ETS\n\n(8 pts) Use the start of the data through March 27, 2021 in order to choose an ARIMA model. You should follow the diagnostic steps that we discussed in class (weeks 9-11: “Autoregressive integrated moving average models”), and it is easiest to do this on the national scale (sum over all state deaths). Fit this ARIMA model to each state, using ARIMA() from the fable package, and evaluate it with time series CV to make 1-4 week ahead forecasts, starting at a target date of April 3, 2021 and going through the end of the data set. (You may still want to implement the time series CV loop manually, instead of relying on stretch_tsibble(), as the latter may be too slow due to the large memory hit.) As before, plot the MASE from time series CV averaged over states, as a function of horizon.\n\n\n# CODE GOES HERE\n\n\n(6 pts) Fit two ETS models, using ETS() from the fable package: Holt’s linear trend and damped linear trend (each with additive errors). Evaluate them using time series CV, just as in Q7, plotting the state-averaged MASE as a function of horizon. Compare and discuss the MASE curves you have seen for all methods thus far.\n\n\n# CODE GOES HERE\n\n\n(4 pts) Compute the coverage of the 80% prediction intervals from the ARIMA and ETS forecasting models from Q7 and Q8, averaged over all states, as a function of horizon.\n\n\n# CODE GOES HERE\n\n\n(Bonus) Plot coverage for the ARIMA and ETS models from Q7 and Q8 a function of target date, averaged over all states, for each of 1-4 week ahead forecasts. (So you should end up with 4 plots, and 3 coverage curves per plot: 1 curve per model.) When do large dips in coverage occur?\n\n\n# CODE GOES HERE\n\n\n\nBonus: geo pooling\n\n(Bonus) Refit the AR and ARX models from Q5, with trailing training windows, but now pooling the data over all states. Thus, for each of 1-4 week ahead forecasting, you will just fit a single AR or ARX model. In order for this to make sense (as case and death counts from the states are on different scales, due to population differences), you will need to standardize the training data from each state before pooling it together to form one big training set, at each step of time series CV. By standardize, we mean center the data to have mean zero, and scale it to have unit variance (do this separately for cases and deaths). Making predictions will be a bit more tricky: you will need to save the centering and scaling factors so that you can then transform back to the proper scale (to the original scale of any given state). How do the MASE curves look like from these geo-pooled models, as a function of horizon?\n\n\n# CODE GOES HERE\n\n\n\nBonus: other forecasters\n\n(Bonus) Fit whatever other forecasters you want, and evaluate them with time series CV, over the same time period as in the previous questions. Make sure you only use the winter 2020 wave for diagnostic and exploratory work. Explain why you chose the models that you did, and discuss the MASE curves.\n\n\n# CODE GOES HERE\n\n\n\nEnsembling\n\n(6 pts) For each of 1-4 week ahead forecasts, ensemble the predictions from the models you have fit thus far (2 AR models, 2 ARX models, 1 ARIMA model, 2 ETS models, plus any other forecasters you may have fit in Q12). Do so in two ways: take an average or median of forecasts (always for a single target). Compare the MASE of these ensemble models from time series CV on the same time period as in the previous questions. What do you see?\nNow instead of simply reporting the average MASE over all states, plot the whole distribution of MASE values over all states (as a histogram), per horizon. Compare the ensemble models to one or two of the previous models (say the AR and ARX models). What do you see?\n\n\n# CODE GOES HERE\n\n\n(Bonus) Finally, compare the MASE and coverage of the COVIDhub ensemble model, whose 1-4 week ahead forecasts can be found in the forecasts data frame, to those from your ensembles. Recall that the COVIDhub ensemble was created in real-time with only access to partial data (subject to revision). How does it compare?\n\n\n# CODE GOES HERE"
  },
  {
    "objectID": "homeworks/homework3/homework3.html",
    "href": "homeworks/homework3/homework3.html",
    "title": "Homework 3: [YOUR NAME HERE]",
    "section": "",
    "text": "The total number of points possible for this homework is 38. The number of points for each question is written below, and questions marked as “bonus” are optional. Submit the knitted html file from this Rmd to Gradescope.\nIf you collaborated with anybody for this homework, put their names here:\n\nRegression troubles\n\n(5 pts) Suppose that \\(y \\in \\mathbb{R}^n\\) is a response vector and \\(X \\in \\mathbb{R}^{n\n\\times p}\\) is a predictor matrix, with \\(p &gt; n\\). Prove that there is at least one \\(\\eta \\not= 0\\) (not equal to the zero vector) that is in \\(\\mathrm{null}(X)\\), the null space of \\(X\\). Prove that if \\(\\tilde\\beta\\) is a least squares solution in the regression of \\(y\\) on \\(X\\), then any vector of the form [ = + , ] is also a solution.\n\nSOLUTION GOES HERE\n\n(6 pts) With \\(X, y\\) as in Q1, suppose that \\(\\tilde\\beta\\) is a least squares solution with \\(\\tilde\\beta_j &gt; 0\\), and suppose that \\(\\mathrm{null}(X) \\not\\perp e_j\\), where \\(e_j\\) is the \\(j^{\\text{th}}\\) standard basis vector (i.e., \\(e_j\\) is a vector with all 0s except for a 1 in the \\(j^{\\text{th}}\\) component), and recall we write \\(S \\perp v\\) for a set \\(S\\) and vector \\(v\\) provided \\(u^T v = 0\\) for all \\(u \\in S\\). Prove that there exists another least squares solution \\(\\hat\\beta\\) such that \\(\\hat\\beta_j &lt; 0\\).\n\nSOLUTION GOES HERE\n\n\nRidge and lasso\n\n(8 pts) On the cardiovascular mortality regression data, form lagged features from the particulate matter and temperature variables, using lags 4, 8, …, 40 from each. Using the glmnet package, fit a ridge regression and lasso regression (two separate models), each over a grid of tuning parameter values \\(\\lambda\\) chosen by the glmnet() function, with cardiovascular mortality as the response and all the lagged features as predictors (you should have 20 in total: 10 from particulate matter, and 10 from temperature). However, make sure you do this in a split-sample setup for validation, as follows, for each of ridge and lasso:\n\n\nfit the glmnet object on the first half of the time series;\nmake predictions on the second half of the time series, for each \\(\\lambda\\);\nrecord the MAE of the predictions on the second half, for each \\(\\lambda\\);\nchoose and report the value of \\(\\lambda\\) with the lowest MAE;\nplot the cardiovascular mortality time series, along with the predictions on the second half, and print the MAE and the selected value of \\(\\lambda\\) on the plot.\nYou can build off the code given in the regularization lecture (weeks 5-6: “Regularization and smoothing”) for fitting the ridge and lasso models, and the regression lecture (weeks 3-4: “Regression and prediction”) for the split-sample validation. Note carefully that the lecture code includes lag 0, and here we do not, so that we can make ex-ante 4-week ahead forecasts!\n\n\n# CODE GOES HERE\n\n\n(1 pts) Which lagged features were present in the MAE-optimal lasso model, selected by split-sample validation, in Q3?\n\n\n# CODE GOES HERE\n\n\n(8 pts) Repeat Q3, except implement time series cross-validation instead of split-sample validation. You should begin time series cross-validation on the second half of the time series, treating the first half as a burn-in set. Also, be sure to fit each ridge or lasso model using a trailing window of 200 time points (not all past).\nWarning: doing time series cross-validation properly will require us to pay attention to the following. The glmnet() function chooses a sequence of tuning parameter values \\(\\lambda\\) based on the passed feature matrix x and response vector y (its first two arguments). However, in time series cross-validation, this will change at each time point. So if you just call glmnet() naively, then you will not have a consistent \\(\\lambda\\) sequence over which to calculate MAE and perform tuning.\nYou can circumvent this issue by defining your own \\(\\lambda\\) sequence ahead of time, and forcing glmnet() to use it by passing it through its lambda argument. Indeed, the best thing to do here is just to use the lambda sequence that glmnet() itself derived for the ridge and lasso models fit to the first half of the time series, which you already have from Q3. Do this, and then just as in Q3, produce a plot of the cardiovascular mortality time series, along with the predictions from time series CV on the second half, and print the MAE and the selected value of \\(\\lambda\\) on the plot.\nYou can build off the code given in the regression lecture for time series cross-validation (or the code you wrote in Homework 2 to implement time series cross-validation).\n\n\n# CODE GOES HERE\n\n\n(Bonus) Report which lagged features were most frequently selected by the lasso. Because the lasso models will be refit at each time point (in the second half of the data set), you will have to additionally store the lasso solutions along your time series CV pass. Then, look back at the solutions that correspond to the MAE-optimal \\(\\lambda\\) value, and choose some way of summarizing which of its components were consistently large in magnitude over time.\n\n\n# CODE GOES HERE\n\n\n\nHP filter\n\n(5 pts) Recall in lecture we saw the HP filter could be written explicitly as [ = K , y, ] where \\(D \\in \\mathbb{R}^{(n-2) \\times n}\\) is the second difference matrix on \\(n\\) points. In other words, defining \\(K \\in \\mathbb{R}^{n \\times n}\\) as above, [ i = {j=1}^n K{ij} y_j, i = 1,,n. ] Compute the matrix \\(K\\) empirically for a problem of size \\(n=100\\), and setting the tuning parameter to be \\(\\lambda=100\\); inspect three of its rows, at indices \\(i = 25, 50, 75\\). For each \\(i\\), plot the \\(i^{\\text{th}}\\) row as a curve over the underlying position \\(1,\\dots,n\\); that is, plot the x-y pairs [ (x_j, y_j) = (j, K_{ij}), j = 1,,n ] as a curve. Overlay the curves for all three rows on the same plot, each in a different color. What do these curves look like to you? Use the plot to argue that the HP filter acts like a kernel smoother.\n\n\n# CODE GOES HERE\n\n\n(Bonus) Do a literature search to find theory on the asymptotically equivalent kernel for the HP filter. This should have a closed-form. Plot this and comment on whether or not your empirical results adhere to what is known asymptotically.\n\n\n# CODE GOES HERE\n\n\n\nTrend filter\n\n(Bonus) Implement 5-fold cross-validation in order to tune \\(\\lambda\\) in trend filtering applied to the Boston marathon men’s data set. Recall the description of how to set folds in a special “structured” way, for tuning smoothers, given near the end of the lecture notes (weeks 5-6: “Regularization and smoothing”). The code below shows how to run trend filtering and derive estimates at the held-out points for one fold. You can build off this code for your solution. You will need to install the glmgen package from GitHub, which you can do using the code that has been commented out.\nImportant note: just like glmnet(), the trendfilter() function (in the glmnet package) computes its own lambda sequence. So you will need to define an initial lambda sequence to pass to each subsequent call to trendfilter(), so that you can have a consistent grid of tuning parameter values over which to perform cross-validation. We do this below by using the lambda sequence that trendfilter() itself derived when the trend filtering model is fit on the full data set.\nAfter implementing cross-validation, compute and report the \\(\\lambda\\) value with the smallest cross-validated MAE. Then, lastly, plot the solution at this value of \\(\\lambda\\) when the model is fit to the full data set (this is already available in the tf object below.)\n\n\n# devtools::install_github(\"glmgen/glmgen\", subdir = \"R_pkg/glmgen\")\nlibrary(glmgen)\nlibrary(fpp3)\n\nRegistered S3 method overwritten by 'tsibble':\n  method               from \n  as_tibble.grouped_df dplyr\n\n\n── Attaching packages ──────────────────────────────────────────── fpp3 1.0.0 ──\n\n\n✔ tibble      3.2.1     ✔ tsibble     1.1.5\n✔ dplyr       1.1.4     ✔ tsibbledata 0.4.1\n✔ tidyr       1.3.0     ✔ feasts      0.3.2\n✔ lubridate   1.9.3     ✔ fable       0.3.4\n✔ ggplot2     3.5.1     ✔ fabletools  0.4.2\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\nboston = boston_marathon |&gt;\n  filter(Year &gt;= 1924) |&gt;\n  filter(Event == \"Men's open division\") |&gt;\n  mutate(Minutes = as.numeric(Time)/60) |&gt;\n  select(Year, Minutes)\n\n# Fit trend filtering on the entire data in order to grab the lambda sequence\ntf = trendfilter(x = boston$Year, y = boston$Minutes, k = 1)\nlambda = tf$lambda\n\nn = nrow(boston)       # Number of points\nk = 5                  # Number of folds\ninds = rep_len(1:k, n) # Folds indices\n\n# Fit trend filtering on all points but those in first fold. We are forcing it\n# to use the lambda sequence that we saved above\ntf_subset = trendfilter(x = boston$Year[inds != 1], \n                        y = boston$Minutes[inds != 1], \n                        k = 1, lambda = lambda)\n\n# Compute the predictions on the points in the first fold. Plot the predictions\n# (as a sanity check) at a particular value of lambda in the middle of the grid\nyhat = predict(tf_subset, x.new = boston$Year[inds == 1])\n\nWarning: In predict: \n    Predict called at new x values out of the original range.\n\nplot(boston$Year, boston$Minutes, col = 8)\npoints(boston$Year[inds == 1], yhat[, 25], col = 2, pch = 19)\n\n\n\n\n\n\n\n\n\n\nSpectral analysis\n\n(3 pts) Let \\(\\omega_j\\), \\(j = 1,\\dots,p\\) be fixed and arbitrary frequencies and let \\(U_{j1}, U_{j2}\\), \\(j = 1,\\dots,p\\) be uncorrelated random variables with mean zero, where \\(U_{j1}, U_{j2}\\) have variance \\(\\sigma^2_j\\). Define [ x_t = {j=1}^p ( U{j1} (2j t) + U{j2} (2_j t) ) ]\nfor \\(t = 1,2,3,\\dots\\). Prove that this process is stationary, and show that its auto-covariance function is of the form given in lecture (weeks 7-8, “Spectral analysis and filtering”).\n\nSOLUTION GOES HERE\n\n(2 pts) Construct a small empirical example to verify the auto-covariance formula you derived in Q10. That is, generate a process with at least \\(p=2\\) components. compute its auto-correlation function with acf(), and compare to the analytic formula you derived.\n\n\n# CODE GOES HERE"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics 153: Introduction to Time Series",
    "section": "",
    "text": "Syllabus\n\n  GitHub\n\n  Ed Discussion\n\n  bCourses\n\n\nNo matching items\n\n\n\n\n\n\n\n\n[UNDER CONSTRUCTION]\n\nInstructor: Ryan Tibshirani (ryantibs at berkeley dot edu)\nGSI: Alice Cima (alice_cima at berkeley dot edu)\nReader: David Zhang (dzhang2324 at berkeley dot edu)\n\nPlease email the GSI with any issues; the Instructor will be looped in only as-needed.\n\nClass times: Mon, Weds, Fri, 1-2pm, SSB 20\nLab times: Fri, 2-4p and 4-6pm, Evans 332\nOffice hours: RT: Mon, 2-3pm, Evans 417\n\nHandy links:\n\nSyllabus\nGitHub repo (source files for lectures and homeworks)\nEd Discussion (for class discussions and announcements)\nbCourses (for grade-keeping and homework solutions)\n\n\n\n\n\n\n\n\nSchedule\n\n\n\nWeek 1: Aug 23 - Aug 25\nCharacteristics of time series\npdf, html, source\n\n\n\nWeek 2: Aug 28 - Sep 1\nMeasures of dependence\npdf, html, source\n\n\n\nWeek 3: Sep 6 - Sep 8\nRegression and prediction\npdf, html, source\nHw 1 due Thur Sep 7\n\n\nWeek 4: Sep 11 - Sep 15\nRegression and prediction\n”\n\n\n\nWeek 5: Sep 18 - Sep 22\nRegularization and smoothing\npdf, html, source\nHw 2 due Thur Sep 21\n\n\nWeek 6: Sep 25 - Sep 29\nRegularization and smoothing\n”\n\n\n\nWeek 7: Oct 2 - Oct 6\nSpectral analysis\npdf, html, source\nHw 3 due Fri Oct 6\n\n\nWeek 8: Oct 9 - Oct 13\nSpectral analysis\n”\n\n\n\nWeek 9: Oct 16 - Oct 20\nARIMA models\npdf, html, source\nMidterm Fri Oct 20\n\n\nWeek 10: Oct 23 - Oct 27\nARIMA models\n”\n\n\n\nWeek 11: Oct 30 - Nov 3\nARIMA models\n”\n\n\n\nWeek 12: Nov 6 - Nov 8\nETS models\npdf, html, source\nHw 4 due Mon Nov 6\n\n\nWeek 13: Nov 13 - Nov 17\nETS models\n”\n\n\n\nWeek 14: Nov 20\n[Nothing! Enjoy Thanksgiving]\n\n\n\n\nWeek 15: Nov 27 - Dec 1\nAdvanced topics\npdf, html, source\nHw 5 due Fri Dec 1, Final Weds Dec 13\n\n\n\n\n\nHomework\n\n\n\nHomework 1\nRmd, html, source\n\n\nHomework 2\nRmd, html, source\n\n\nHomework 3\nRmd, html, source\n\n\nHomework 4\nRmd, html, source\n\n\n\nHomework 5 has been canceled; see bCourses for new Homework 5.\n\n\nSupplementary resources\nWe will (roughly) follow some chapters of the following two books, which you can look at as supplements to the lecture notes. The first should be available to you by searching for it online through the UC Berkeley Library, and the second is freely available at the link below.\n\nShumway, Stoffer, Time Series Analysis and Its Applications, 2017.\nHyndman, Athanasopoulos, Forecasting: Principles and Practice, 2021.\n\nBelow are two other references on time series that may be helpful as well. The first is more advanced, and the second more elementary.\n\nBrockwell, David, Introduction to Time Series and Forecasting, 2016.\nCryer, Chan, Time Series Analysis With Applications in R, 2008.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "homeworks/homework1/homework1.html",
    "href": "homeworks/homework1/homework1.html",
    "title": "Homework 1: [YOUR NAME HERE]",
    "section": "",
    "text": "The total number of points possible for this homework is 41. The number of points for each question is written below, and questions marked as “bonus” are optional. Submit the knitted html file from this Rmd to Gradescope.\nIf you collaborated with anybody for this homework, put their names here:\n\nCorrelation and independence\n\n(3 pts) Give an example to show that two random variables can be uncorrelated but not independent. You must explicitly prove that they are uncorrelated but not independent (for the latter, you may invoke any property that you know is equivalent to independence).\n\nSOLUTION GOES HERE\n\n(2 pts) If \\((X,Y)\\) has a multivariate Gaussian distribution, and \\(X,Y\\) are uncorrelated: \\(\\mathrm{Cov}(X,Y) = 0\\), then show that \\(X,Y\\) are independent.\n\nSOLUTION GOES HERE\n\n(3 pts) Give an example to show that two random variables \\(X,Y\\) can be marginally Gaussian (meaning, \\(X\\) is Gaussian, and \\(Y\\) is Gaussian) and uncorrelated but not independent. Hint: \\((X,Y)\\) cannot be multivariate Gaussian in this case.\n\nSOLUTION GOES HERE\n\n\nRandom walks\n\n(4 pts) Let \\(x_t\\), \\(t = 1,2,3,\\dots\\) be a random walk with drift: [ x_t = + x_{t-1} + _t, ] where (say) \\(\\epsilon_t \\sim N(0,\\sigma^2)\\) for \\(t = 1,2,3,\\dots\\). Suppose that both \\(\\delta\\) and \\(\\sigma^2\\) are unknown. Devise a test statistic for the null hypothesis that \\(\\delta = 0\\). This should be based on a standard test that you know (have learned in a past course) for testing whether the mean of Gaussian is zero, with unknown variance, based on i.i.d. samples from this Gaussian.\nState what the null distribution is for this test statistic, and how you would compute it in R (a function name is sufficient if the test statistic is implemented as a function in base R). Hint: consider taking differences along the sequence … after that, what you want sounds like “c-test”, or “p-test”, or “\\(\\phi\\)-test”, or …\n\nSOLUTION GOES HERE\n\n(2 pts) Simulate a random walk of length 100 without drift, i.e., \\(\\delta = 0\\), and compute the test statistic you devised in Q4 and report its value. Then repeat, but using a large nonzero value \\(\\delta\\).\n\n\n# CODE GOES HERE\n\n\n(4 pts) Simulate 100 random walks each of length 500, with nonzero drift, and plot them on the same plot using transparent coloring, following the code used in the lecture notes from week 2 (“Measures of dependence and stationarity”). Calculate the sample mean \\(\\hat\\mu_t\\) at each time \\(t\\), across the repetitions, and plot as a dark line on the same plot. Then, calculate the sample standard deviation \\(\\hat\\sigma_t\\) at each time \\(t\\), and plot the mean plus or minus one standard deviation: \\(\\hat\\mu_t \\pm \\hat\\sigma_t\\), as dark dotted lines on the same plot. Describe what you see (you should see that both the mean and variance increase over time).\n\n\n# CODE GOES HERE\n\n\n\nStationarity\n\n(3 pts) Compute the mean, variance, auto-covariance, and auto-correlation functions for the process [ x_t = w_t w_{t-1}, ] where each \\(w_t \\sim N(0, \\sigma^2)\\), independently. Is \\(x_t\\), \\(t = 1,2,3,\\dots\\) stationary?\n\nSOLUTION GOES HERE\n\n(3 pts) Repeat the same calculations in Q7, but where each \\(w_t \\sim N(\\mu, \\sigma^2)\\), independently, for \\(\\mu \\not= 0\\). Is \\(x_t\\), \\(t = 1,2,3,\\dots\\) stationary?\n\nSOLUTION GOES HERE\n\n(3 pts) Simulate the processes from Q7 (with \\(\\mu = 0\\)) and Q8 (with \\(\\mu \\not= 0\\)), yielding two time series of length 500, and plot the results. Compute the sample mean and sample variance for each one (to be clear, this just a sample mean of all data, over all time, and similarly for the variance), and check that these are close to the population mean and variance from Q7 and Q8. Also compute and plot the sample auto-correlation function using acf(), and check again that it agrees with the population auto-correlation function from Q7 and Q8.\n\n\n# CODE GOES HERE\n\n\n(2 pts) Give an example of a weakly stationary process that is not strongly stationary.\n\nSOLUTION GOES HERE\n\n(Bonus) A function \\(\\kappa\\) is said to be positive semidefinite (PSD) provided that [ _{i,j=1}^n a_i a_j (t_i - t_j) , . ] Prove that is \\(x_t\\), \\(t = 1,2,3,\\dots\\) is stationary, and \\(\\gamma_x(h)\\) is its auto-covariance function (as a function of lag \\(h\\)), then \\(\\gamma_x\\) is PSD. You may use any equivalences between PSD and other linear-algebraic properties that you want, as long as you state clearly what you are using.\n\nSOLUTION GOES HERE\n\n\nJoint stationarity\nNotions of joint stationarity, between two time series, can be defined in an analogous way to how we defined stationarity in lecture. We say that two time series \\(x_t\\), \\(t = 1,2,3,\\dots\\) and \\(y_t\\), \\(t = 1,2,3,\\dots\\) are strongly jointly stationary provided that: \\[\\begin{multline*}\n(x_{s_1}, x_{s_2}, \\dots, x_{s_k}, y_{t_1}, y_{t_2}, \\dots, y_{t_\\ell})\n\\overset{d}{=}(x_{s_1+h}, x_{s_2+h}, \\dots, x_{s_k+h}, y_{t_1+h}, y_{t_2+h}, \\dots,\ny_{t_\\ell+h}), \\\\ \\text{for all $k,\\ell \\geq 1$, all $s_1,\\dots,s_k$ and\n$t_1,\\dots,t_\\ell$, and all $h$}.\n\\end{multline*}\\] Here \\(\\overset{d}{=}\\) means equality in distribution. In other words, any collection of variates from the two sequences has the same joint distribution after we shift the time indices forward or backwards in time. Meanwhile, we say that \\(x_t\\), \\(t = 1,2,3,\\dots\\) and \\(y_t\\), \\(t = 1,2,3,\\dots\\) are weakly jointly stationary or simply jointly stationary provided that each series is stationary, and: [ {xy}(s,t) = {xy}(s+h, t+h), . ] Here \\(\\gamma_{xy}\\) is the cross-covariance function between \\(x,y\\). In other words, the cross-covariance function must be invariant to shifts forward or backwards in time, and is only a function of the lag \\(h = s-t\\). For jointly stationary series, we can hence abbreviate their cross-covariance function by \\(\\gamma_{xy}(h)\\).\n\n(2 pts) Give an example of two time series that are weakly jointly stationary but not strongly jointly stationary.\n\nSOLUTION GOES HERE\n\n(3 pts) If \\(x_t\\), \\(t = 1,2,3,\\dots\\) and \\(y_t\\), \\(t = 1,2,3,\\dots\\) form a joint Gaussian process, which means that any collection \\((x_{s_1}, x_{s_2}, \\dots, x_{s_k},\ny_{t_1}, y_{t_2}, \\dots, y_{t_\\ell})\\) of variates along the series has a multivariate Gaussian distribution, then prove that weak joint stationarity implies strong joint stationarity.\n\nSOLUTION GOES HERE\n\n(3 pts) Write down explicit formulas that shows how to estimate the cross-covariance and cross-correlation function of two finite time series \\(x_t\\), \\(t = 1,\\dots,n\\) and \\(y_t\\), \\(t = 1,\\dots,n\\), under the assumption of joint stationarity. Hint: these should be entirely analogous to the sample auto-covariance and sample auto-correlation functions that we covered in lecture.\n\nSOLUTION GOES HERE\n\n(4 pts) Following the code used in the lecture notes from week 2 (“Measures of dependence and stationarity”), use the ccf() function to compute and plot the sample cross-correlation function between Covid-19 cases and deaths, separately, for each of Florida, Georgia, New York, Pennsylvania, and Texas. (The lecture code does this for California.) Comment on what you find: do the cross-correlation patterns look similar across different states?\nAlso, follow the lecture code to plot the case and death signals together, on the same plot, for each state (the lecture code provides a way to do this so that they are scaled dynamically to attain the same min and max, and hence look nice when plotted together). Comment on whether the estimated cross-correlation patterns agree with what you see visually between the case and death signals.\n\n\n# CODE GOES HERE"
  },
  {
    "objectID": "homeworks/homework2/homework2.html",
    "href": "homeworks/homework2/homework2.html",
    "title": "Homework 2: [YOUR NAME HERE]",
    "section": "",
    "text": "The total number of points possible for this homework is 35. The number of points for each question is written below, and questions marked as “bonus” are optional. Submit the knitted html file from this Rmd to Gradescope.\nIf you collaborated with anybody for this homework, put their names here:\n\nSimple regression\n\n(2 pts) Derive the population least squares coefficients, which solve [ _{_1, _0} , , ] by differentiating the criterion with respect to each \\(\\beta_j\\), setting equal to zero, and solving. Repeat the calculation but without intercept (without the \\(\\beta_0\\) coefficient in the model).\n\nSOLUTION GOES HERE\n\n(2 pts) As in Q1, now derive the sample least squares coefficients, which solve [ _{_1, 0} , {i=1}^n (y_i - _0 - _1 x_i)^2. ] Again, repeat the calculation but without intercept (no \\(\\beta_0\\) in the model).\n\nSOLUTION GOES HERE\n\n(2 pts) Prove of disprove: in the model without intercept, is the regression coefficient of \\(x\\) on \\(y\\) the inverse of that from the regression of \\(y\\) on \\(x\\)? Answer the question for each of the the population and sample versions.\n\nSOLUTION GOES HERE\n\n(3 pts) Consider the following hypothetical. Let \\(y\\) be the height of a child and \\(x\\) be the height of their parent, and consider a regression of \\(y\\) on \\(x\\), performed in in a large population. Suppose that we estimate the regression coefficients separately for male and female parents (two separate regressions) and we find that the slope coefficient from the former regression \\(\\hat\\beta_1^{\\text{dad}}\\) is smaller than that from the latter \\(\\hat\\beta_1^{\\text{mom}}\\). Suppose however that we find (in this same population) the sample correlation between a father’s height and their child’s height is larger than that between a mother’s height and their child’s height. What is a plausible explanation for what is happening here?\n\nSOLUTION GOES HERE\n\n\nMultiple regression\n\n(2 pts) In class, we claimed that the multiple regression coefficients, with respect to responses \\(y_i\\) and feature vectors \\(x_i \\in \\mathbb{R}^p\\), \\(i = 1,\\dots,n\\), can be written in two ways: the first is [ = ( {i=1}^n x_i x_i^T )^{-1} {i=1}^n x_i y_i. ] The second is [ = (X^T X)^{-1} X^T y, ] where \\(X \\in \\mathbb{R}^{n \\times p}\\) is a feature matrix, with \\(i^{\\text{th}}\\) row \\(x_i\\), and \\(y \\in \\mathbb{R}^n\\) is a response vector, with \\(i^{\\text{th}}\\) component \\(y_i\\). Prove that these two expressions are equivalent.\n\nSOLUTION GOES HERE\n\n(Bonus) Derive the population and sample multiple regression coefficients by solving the corresponding least squares problem (differentiating the criterion with respect to each \\(\\beta_j\\), setting equal to zero, and solving). For the sample least squares coefficient, deriving either representation in Q5 will be fine.\n\nSOLUTION GOES HERE\n\n\nMarginal-multiple connection\n\n(1 pts) Consider the simple linear regression of a generic response \\(y_i\\) on a constant predictor \\(x_i = 1\\), \\(i = 1,\\dots,n\\), without intercept. Give the exact form of the sample regression coefficient.\n\nSOLUTION GOES HERE\n\n(3 pts) Recall the connection between multiple and marginal regression coefficients, as covered in lecture: the \\(j^{\\text{th}}\\) multiple regression coefficient can be written in general as [ _j = {(^{-j}_j)^T ^{-j}_j}, ] which we interpret as the simple linear regression coefficient of \\(\\hat{y}^{-j}\\) on \\(\\hat{x}^{-j}\\). These are \\(y\\) and \\(x_j\\), respectively, after we regress out the contributions of all other features. (See the lecture notes for the precise details.)\nNow note that we can treat a simple linear regression with an intercept term as a multiple regression with two features, with the first feature just equal to the constant 1. Using the above formula, and the answer from Q7, re-derive the expression for the slope in the simple linear model with intercept: [ 1 = {{i=1}^n (x_i - {x})^2}. ]\n\nSOLUTION GOES HERE\n\n\nCovariance calculations\n\n(3 pts) Let \\(x \\in \\mathbb{R}^n\\) and \\(y \\in \\mathbb{R}^m\\) be random vectors, and let \\(A \\in \\mathbb{R}^{k \\times n}\\) and \\(B \\in \\mathbb{R}^{\\ell \\times m}\\) be fixed matrices. Prove that [ (Ax, By) = A (x, y) B^T. ] Prove as a consequence that \\(\\mathrm{Cov}(Ax) = A \\mathrm{Cov}(x) A^T\\). Hint: you may use the rule for covariances of linear combinations (as reviewed in the lecture from week 2, “Measures of dependence and stationarity”).\n\nSOLUTION GOES HERE\n\n(2 pts) Suppose that \\(y = X \\beta + \\epsilon\\), with \\(X\\) and \\(\\beta\\) fixed, and where \\(\\epsilon\\) is a vector with white noise entries, with variance \\(\\sigma^2\\). Use the rule in Q9 to prove that for the sample least squares coefficients, namely, \\(\\hat\\beta = (X^T X)^{-1} X^T y\\), it holds that [ () = ^2 (X^T X)^{-1}. ]\n\nSOLUTION GOES HERE\n\n(4 pts) An equivalent way to state the Gauss-Markov theorem is as follows. Under the model from Q10, if \\(\\tilde\\beta\\) is any other unbiased linear estimator of \\(\\beta\\) (where linearity means that \\(\\tilde\\beta = My\\) for a fixed matrix \\(M\\)) then [ () () ] where \\(\\lesssim\\) means less than or equal to in the PSD (positive semidefinite) ordering. Precisely, \\(A \\lesssim B\\) if and only if \\(B-A\\) is a PSD matrix, which recall, means \\(z^T (B-A) z \\geq 0\\) for all vectors \\(z\\). Prove that this is indeed equivalent to the statement of the Gauss-Markov theorem given in lecture.\n\nSOLUTION GOES HERE\n\n\nCross-validation\n\n(3 pts) Recall the R code from lecture that performs time series cross-validation to evaluate the mean absolute error (MAE) of predictions from the linear regression of cardiovascular mortality on 4-week lagged particulate levels. Adapt this to evaluate the MAE of predictions from the regression of cardiovascular mortality on 4-week lagged particulate levels and 4-week lagged temperature (2 features). Fit each regression model using a trailing window of 200 time points (not all past). Plot the predictions, and print the MAE on the plot, following the code from lecture.\nAdditionally (all drawn on the same figure), plot the fitted values on the training set. By the training set here, we mean what is also called the “burn-in set” in the lecture notes, and indexed by times 1 through t0 in the code. The fitted values should come from the initial regression model that is fit to the burn-in set. These should be plotted in a different color from the predictions made in time series cross-validation pass. Print the MAE from the fitted values the training set somewhere on the plot (and label this as “Training MAE” to clearly differentiate it).\n\n\n# CODE GOES HERE\n\n\n(2 pts) Repeat the same exercise as in Q12 but now with multiple lags per variable: use lags 4, 8, 12 for each of particulate level and temperature (thus 6 features in total). Did the training MAE go down? Did the cross-validated MAE go down? Discuss. Hint: you may find it useful to know that lm() can take a predictor matrix, as in lm(y ~ x) where x is a matrix; in this problem, you can form the predictor matrix by calling cbind() on the lagged feature vectors.\n\n\n# CODE GOES HERE\n\n\n(2 pts) Repeat once more the same exercise as in the last question but but now with many lags per variable: use lags 4, 5, …, through 50 for each of particulate level and temperature (thus 47 x 2 = 94 features in total). Did the training MAE go down? Did the cross-validated MAE go down? Are you surprised? Discuss.\n\n\n# CODE GOES HERE\n\n\n\nMore features, the merrier?\n\n(2 pts) Let \\(y_i\\) be an arbitrary response, and \\(x_i \\in \\mathbb{R}^p\\) be an arbitrary feature vector, for \\(i = 1,\\dots,n\\). Let [ i = (x{i1}, , x_{ip}, {i,p+1}),i = 1,,n ] be the result of appending one more feature. Let \\(\\hat{y}_i\\) denote the fitted values from the regression of \\(y_i\\) on \\(x_i\\), and let \\(\\tilde{y}_i\\) denote the fitted values from the regression of \\(y_i\\) on \\(\\tilde{x}_i\\). Prove that [ {i=1}^n (y_i - i)^2 {i=1}^n (y_i - _i)^2. ] In other words, the training MSE will never get worse as we add features to a given sample regression problem.\n\nSOLUTION GOES HERE\n\n(2 pts) How many linearly independent features do we need (how large should \\(p\\) be) in order to achieve a perfect training accuracy, i.e., training MSE of zero? Why?\n\nSOLUTION GOES HERE\n\n(Bonus) Implement an example in R in order to verify your answer to Q16 empirically. Extra bonus points if you do it on the cardiovascular mortality data, using enough lagged features. You should be able to plot the fitted values from the training set and see that they match the observations perfectly (and the CV predictions should look super wild).\n\n\n# CODE GOES HERE"
  },
  {
    "objectID": "homeworks/homework4/homework4.html",
    "href": "homeworks/homework4/homework4.html",
    "title": "Homework 4: [YOUR NAME HERE]",
    "section": "",
    "text": "The total number of points possible for this homework is 34. The number of points for each question is written below, and questions marked as “bonus” are optional. Submit the knitted html file from this Rmd to Gradescope.\nIf you collaborated with anybody for this homework, put their names here:\n\nBackshift commuting\n\n(1 pt) Let \\(B\\) denote the backshift operator. Given any integers \\(k,\\ell \\geq 0\\), explain why \\(B^k B^\\ell = B^\\ell B^k\\).\n\nSOLUTION GOES HERE\n\n(2 pts) Using Q1, if \\(\\phi_1,\\dots,\\phi_k\\) and \\(\\varphi_1,\\dots,\\varphi_\\ell\\) are any coefficients, show that [ (1 + _1 B + + _k B^k) (1 + 1 B + + B^) = (1 + 1 B + + B^) (1 + _1 B + + _k B^k) ]\n\nSOLUTION GOES HERE\n\n(2 pts) Verify the result in Q2 with a small code example.\n\n\n# CODE GOES HERE\n\n\n(3 pts) Using Q2, show that we can write a SARIMA model equivalently as [ (B) (B^s) ^d _s^D x_t = (B) (B^s) w_t ] and [ (B^s) (B) _s^D ^d x_t = (B^s) (B) w_t. ]\n\nSOLUTION GOES HERE\n\n\nLong-range ARIMA\n\n(1 pt) Let \\(\\nabla = (1 - B)\\) denote the difference operator. Suppose that \\(\\nabla x_t\n= 0\\) for all \\(t\\). Prove that \\(x_t\\) must be a constant sequence.\n\nSOLUTION GOES HERE\n\n(2 pts) Suppose that \\(\\nabla x_t = u\\) for all \\(t\\), where \\(u\\) is an arbitrary constant. Prove that \\(x_t\\) must be a linear function of \\(t\\), of the form \\(x_t = a + bt\\).\n\nSOLUTION GOES HERE\n\n(3 pts) Suppose that \\(\\nabla x_t = u + vt\\) for all \\(t\\), where \\(u,v\\) are again arbitrary constants. Prove that \\(x_t\\) must be a quadratic function of \\(t\\), of the form \\(x_t = a + bt + ct^2\\).\n\nSOLUTION GOES HERE\n\n(1 pt) Using Q7, prove that if \\(\\nabla^2 x_t = u\\) for all \\(t\\), where \\(u\\) is a constant, then \\(x_t\\) must be a quadratic function of \\(t\\).\n\nSOLUTION GOES HERE\n\n(4 pts) Consider an ARMA(1,1) model: [ (1 - B) x_t = (1 + B) w_t. ] Suppose our estimates pass the “unit root test”, \\(|\\hat\\phi|, |\\hat\\theta| &lt; 1\\), which will be assumed implicitly henceforth. Unravel the forecast iteration described in lecture to show that \\(\\hat{x}_{t+h | t}\\) approaches zero as \\(h \\to\n\\infty\\).\n\nSOLUTION GOES HERE\n\n(2 pts) Consider an ARMA(1,1) model, with intercept: [ (1 - B) x_t = c + (1 + B) w_t. ] Unravel the forecast iteration to show that \\(\\hat{x}_{t+h | t}\\) approaches a nonzero constant as \\(h \\to \\infty\\).\n\nSOLUTION GOES HERE\n\n(Bonus) Now consider the extension to ARIMA(\\(1,d,1\\)): [ (1 - B) ^d x_t = c + (1 + B) w_t. ] Use Q5–Q10 to argue the following:\n\n\nIf \\(c = 0\\) and \\(d = 1\\), then \\(\\hat{x}_{t+h | t}\\) approaches a constant as \\(h\n\\to \\infty\\).\nIf \\(c = 0\\) and \\(d = 2\\), then \\(\\hat{x}_{t+h | t}\\) approaches a linear trend as \\(h \\to \\infty\\).\nIf \\(c \\not= 0\\) and \\(d = 1\\), then \\(\\hat{x}_{t+h | t}\\) approaches a linear trend as \\(h \\to \\infty\\).\nIf \\(c \\not= 0\\) and \\(d = 2\\), then \\(\\hat{x}_{t+h | t}\\) approaches a quadratic trend as \\(h \\to \\infty\\).\n\nSOLUTION GOES HERE\n\n\nTime series CV\nWhen we learned time series cross-validation in lecture (weeks 3-4, “Linear regression and prediction”), we implemented it “manually”, by writing a loop in R to iterate over time, rebuild models, and so on. The fable package in R does it differently. It relies on data being stored in a class that is known as a tsibble, which is like a special data frame for time series. You can then use a function called stretch_tsibble() in order to “prepare it” for time series cross-validation. Take a look at what it does with this example:\n\nlibrary(tidyverse)\nlibrary(fpp3)\n\ndat = tsibble(date = as.Date(\"2023-10-01\") + 0:9, \n              value = 1:10 + rnorm(10, sd = 0.25),\n              index = date)\ndat\n\n# A tsibble: 10 x 2 [1D]\n   date       value\n   &lt;date&gt;     &lt;dbl&gt;\n 1 2023-10-01  1.22\n 2 2023-10-02  1.74\n 3 2023-10-03  3.29\n 4 2023-10-04  4.15\n 5 2023-10-05  5.28\n 6 2023-10-06  5.96\n 7 2023-10-07  7.02\n 8 2023-10-08  7.85\n 9 2023-10-09  9.05\n10 2023-10-10  9.63\n\ndat_stretched = dat|&gt; stretch_tsibble(.init = 3) \ndat_stretched\n\n# A tsibble: 52 x 3 [1D]\n# Key:       .id [8]\n   date       value   .id\n   &lt;date&gt;     &lt;dbl&gt; &lt;int&gt;\n 1 2023-10-01  1.22     1\n 2 2023-10-02  1.74     1\n 3 2023-10-03  3.29     1\n 4 2023-10-01  1.22     2\n 5 2023-10-02  1.74     2\n 6 2023-10-03  3.29     2\n 7 2023-10-04  4.15     2\n 8 2023-10-01  1.22     3\n 9 2023-10-02  1.74     3\n10 2023-10-03  3.29     3\n# ℹ 42 more rows\n\n\nWhat this does is it takes the first 3 entries of the time series and assigns them .id = 1. Then it appends the first 4 entries of the time series and assigns them .id = 2. Then it appends the first 5 entries of the time series and assigns them .id = 3, and so on. Downstream, when we go to fit a forecast model with fable, it (by default) will fit a separate model to the data in each level of the .id column. And by making forecasts at a (say) horizon h = 1, these are actually precisely the 1-step ahead forecasts that we would generate in time series CV:\n\ndat_fc = dat_stretched |&gt;\n  model(RW = RW(value ~ drift())) |&gt;\n  forecast(h = 1) \ndat_fc\n\n# A fable: 8 x 5 [1D]\n# Key:     .id, .model [8]\n    .id .model date              value .mean\n  &lt;int&gt; &lt;chr&gt;  &lt;date&gt;           &lt;dist&gt; &lt;dbl&gt;\n1     1 RW     2023-10-04  N(4.3, 0.8)  4.33\n2     2 RW     2023-10-05 N(5.1, 0.37)  5.13\n3     3 RW     2023-10-06 N(6.3, 0.24)  6.30\n4     4 RW     2023-10-07  N(6.9, 0.2)  6.91\n5     5 RW     2023-10-08   N(8, 0.16)  7.98\n6     6 RW     2023-10-09 N(8.8, 0.13)  8.80\n7     7 RW     2023-10-10  N(10, 0.12) 10.0 \n8     8 RW     2023-10-11  N(11, 0.12) 10.6 \n\n\nThe .mean column give us the point forecast. To evaluate these, we could join the original data dat to the point forecasts in dat_fc, and then align by the date column, and compute whatever metrics we wanted. However, there is also a handy function to do all of this for us, called accuracy(). This computes a bunch of common metrics, and here we just pull out the MAE column:\n\naccuracy(dat_fc, dat) |&gt; select(.model, MAE)\n\nWarning: The future dataset is incomplete, incomplete out-of-sample data will be treated as missing. \n1 observation is missing at 2023-10-11\n\n\n# A tibble: 1 × 2\n  .model   MAE\n  &lt;chr&gt;  &lt;dbl&gt;\n1 RW     0.222\n\n\nNow for the questions.\n\n(3 pts) A clear advantage to the above workflow is convenience: we have to write less code. A disadvantage is that it can be inefficient, and in particular, memory inefficient. To see this, consider using this to do time series CV on a sequence with \\(n\\) observations and burn-in time \\(t_0\\). We store this as a tsibble, call it x, with n rows, and then we run stretch_tsibble(x, .init = t0). How many rows does the output have? Derive the answer mathematically (as an explicit formula involving \\(n,t_0\\)), and then verify it with a couple of code examples.\n\nSOLUTION GOES HERE\n\n# CODE GOES HERE\n\n\n(4 pts) Show that the MAE result for the random walk forecasts produced above, on the data in dat, matches the MAE from a manual implementation of time series CV with the same forecaster. (Your manual implementation can build off the code from the regression lecture, and/or from previous homeworks.)\n\n\n# CODE GOES HERE\n\n\n(6 pts) Consider the leisure data set from the HA book, which the code excerpt below (taken from the ARIMA lecture) prepares for us. Use time series CV, implemented using stretch_tsibble(), model(), and forecast(), as described above, to evaluate the MAE of the following four models:\n\n\nARIMA\\((2,1,0)\\)\nARIMA\\((0,1,2)\\)\nARIMA\\((2,1,0)(1,1,0)_{12}\\)\nARIMA\\((0,1,2)(0,1,1)_{12}\\)\n\nThe last models are motivated by the exploratory analysis done in lecture. The first two remove the seasonal component, which should not be very good (since there is clear seasonality in the data). For each model you should use a burn-in period of length 50 (i.e., set .init = 50 in the call to stretch_tsibble()). A key difference in how you implement time series CV to the above examples: you should consider 1-step, 2-step, all the way through 12-step ahead forecasts. But do not worry! This can be handled with an appropriate call to forecast(). For each model, calculate the MAE by averaging over all forecast horizons (1 through 12). Report the results and rank the models by their MAE.\n\nleisure = us_employment |&gt;\n  filter(Title == \"Leisure and Hospitality\", year(Month) &gt; 2000) |&gt;\n  mutate(Employed = Employed/1000) |&gt;\n  select(Month, Employed)\n\n# CODE GOES HERE\n\n\n(Bonus) Break down the MAE for the forecasts made in Q14 by forecast horizon. That is, for each \\(h = 1,\\dots,12\\), calculate the MAE of the \\(h\\)-step ahead forecasts made by each model. Make a plot with the horizon \\(h\\) on the x-axis and MAE on the y-axis, and compare in particular the models ARIMA\\((2,1,0)(1,1,0)_{12}\\) and ARIMA\\((0,1,2)(0,1,1)_{12}\\). Do you see anything interesting happening here in the comparison between their MAE as we vary the horizon \\(h\\)?\n\n\n# CODE GOES HERE\n\n\n(Bonus^2) Evaluate the forecasts made by auto-ARIMA in this time series CV pipeline. Remember, this means that auto-ARIMA will be rerun (yikes!) at each iteration in time series CV. This may take a very long time to run (which is why this is a Bonus^2). If it finishes for you, how does its MAE compare?\n\n\n# CODE GOES HERE"
  },
  {
    "objectID": "lectures/characteristics/characteristics.html",
    "href": "lectures/characteristics/characteristics.html",
    "title": "Lecture 1: Characteristics and Examples of Time Series Data",
    "section": "",
    "text": "Load packages\n\n# The next few lines install packages that have data sets we'll frequently use\n# in this course. You only need to install them once. For installing the third\n# package, {epidatasets}, which is not yet up on CRAN, you'll need {devtools}\n\n# install.packages(\"astsa\") # From the Shumway and Stoffer book\n# install.packages(\"fpp3\") # From the Hyndman and Athanasopoulos book\n# install.packages(\"tidyverse\")\n# install.packages(\"devtools\") \n# devtools::install_github(\"cmu-delphi/epidatasets\") # From the CMU Delphi group\n\nlibrary(tidyverse)\nlibrary(astsa)\nlibrary(fpp3)\nlibrary(epidatasets)\n\n\n\nJohnson & Johnson data\n\nplot(jj, type = \"o\", ylab = \"Quarterly earnings per share\")\n\n\n\n\n\n\n\n\n\n\nGlobal warming data\n\nplot(xglobtemp, type = \"o\", ylab = \"Global temperature deviations\")\n\n\n\n\n\n\n\n\n\n\nSpeech data\n\nplot(speech, type = \"l\", ylab = \"Vocal response\")\n\n\n\n\n\n\n\n\n\n\nfMRI data\n\nmatplot(fmri1[,2:5], type = \"l\", lty = 1, col = 1:4,\n        xlab = \"Time\", ylab = \"BOLD\", main = \"Cortex\")\n\n\n\n\n\n\n\nmatplot(fmri1[,6:9], type = \"l\", lty = 1, col = 1:4,\n        xlab = \"Time\", ylab = \"BOLD\", main = \"Thalamus & cerebellum\")\n\n\n\n\n\n\n\n\n\n\nBoston marathon\n\nboston_marathon |&gt; \n  ggplot(aes(x = Year, y = Time)) + \n  geom_line() + theme_bw()\n\n\n\n\n\n\n\nboston_marathon |&gt; \n  filter(Event == \"Men's open division\") |&gt;\n  ggplot(aes(x = Year, y = Time)) + \n  geom_line() + theme_bw()\n\n\n\n\n\n\n\nboston_marathon |&gt; \n  ggplot(aes(x = Year, y = Time)) + \n  geom_line() + theme_bw() +\n  facet_wrap(vars(Event))\n\n\n\n\n\n\n\n\n\n\nCovid-19 data\n\ncases_deaths_subset |&gt;\n  ggplot(aes(x = time_value, y = case_rate_7d_av, group = geo_value)) +\n  geom_line(aes(color = geo_value)) + \n  labs(x = \"Date\", y = \"Reported Covid-19 cases per 100k people\") +\n  theme_bw() + \n  scale_color_brewer(palette = \"Set1\") +\n  guides(color = guide_legend(nrow = 1)) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\n\n\n\ncases_deaths_subset |&gt;\n  ggplot(aes(x = time_value, y = death_rate_7d_av, group = geo_value)) +\n  geom_line(aes(color = geo_value)) + \n  labs(x = \"Date\", y = \"Reported Covid-19 deaths per 100k people\") +\n  theme_bw() + \n  scale_color_brewer(palette = \"Set1\") +\n  guides(color = guide_legend(nrow = 1)) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\nWhite noise\n\nplot(rnorm(500), type = \"l\", xlab = \"t\", ylab = \"x_t\")\n\n\n\n\n\n\n\n\n\n\nRandom walk\n\nn = 500 \ndelta = 0.2 # drift\n\nx = cumsum(rnorm(n))\ny = cumsum(rnorm(n) + delta)\n\nmatplot(cbind(x, y), type = \"l\", col = 1:2, \n        xlab = \"t\", ylab = \"x_t\")\nlegend(\"topleft\", lty = 1:2, col = 1:2, \n       legend = paste(\"delta =\", c(0, delta)))\n\n\n\n\n\n\n\n\n\n\nSTL decomposition\n\nus_retail_employment &lt;- us_employment |&gt;\n  filter(year(Month) &gt;= 1990, Title == \"Retail Trade\") |&gt;\n  select(-Series_ID)\n\nus_retail_employment |&gt;\n  model(\n    STL(Employed ~ trend(window = 7) +\n                   season(window = \"periodic\"),\n    robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot() + theme_bw()"
  },
  {
    "objectID": "lectures/ets/ets.html",
    "href": "lectures/ets/ets.html",
    "title": "Lecture 7: Exponential Smoothing With Trend and Seasonality",
    "section": "",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(astsa)\nlibrary(fpp3)\nlibrary(epidatasets)\n\n\n\nInternet useage\n\nwww = as_tsibble(WWWusage)\n\n# Fit models\nwww_fit = www |&gt;\n  model(SES = ETS(value ~ error(\"A\") + trend(\"N\") + season(\"N\")),\n        Holt = ETS(value ~ error(\"A\") + trend(\"A\") + season(\"N\")),\n        Damped = ETS(value ~ error(\"A\") + trend(\"Ad\") + season(\"N\")))\n  \n# Make forecasts\nwww_fit |&gt;\n  forecast(h = 10) |&gt;\n  mutate(.model = factor(.model, levels = c(\"SES\", \"Holt\", \"Damped\"))) |&gt;\n  autoplot(www) + \n  labs(x = \"Minute\", y = \"Number of users\",\n       title = \"Internet usage per minute\") +\n  facet_grid(vars(.model)) + theme_bw() +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\n\n\n\n# Inspect fitted coefficients\nwww_fit |&gt; select(SES) |&gt; coef()\n\n# A tibble: 2 × 3\n  .model term  estimate\n  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1 SES    alpha     1.00\n2 SES    l[0]     87.7 \n\nwww_fit |&gt; select(Holt) |&gt; coef()\n\n# A tibble: 4 × 3\n  .model term  estimate\n  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1 Holt   alpha     1.00\n2 Holt   beta      1.00\n3 Holt   l[0]     85.1 \n4 Holt   b[0]      3.11\n\nwww_fit |&gt; select(Damped) |&gt; coef()\n\n# A tibble: 5 × 3\n  .model term  estimate\n  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1 Damped alpha   1.00  \n2 Damped beta    0.997 \n3 Damped phi     0.815 \n4 Damped l[0]   90.4   \n5 Damped b[0]   -0.0173\n\n\n\n\nAustralian holiday travel\n\nholiday = tourism |&gt;\n  filter(Purpose == \"Holiday\") |&gt;\n  summarize(Trips = sum(Trips)/1e3)\n\n# Fit Holt-Winters model\nholiday_fit = holiday |&gt;\n  model(HoltWinters = ETS(Trips ~ error(\"A\") + trend(\"A\") + \n                            season(\"A\", period = 4)))\n\n# Make forecasts\nholiday_fit |&gt; forecast(h = \"3 years\") |&gt;\n  autoplot(holiday) +\n  labs(y = \"Overnight trips (millions)\",\n       title =\" Australian holiday travel\") + theme_bw()\n\n\n\n\n\n\n\n\n\n\nETS decomposition\n\n# Fit additive and multiplicative Holt-Winters  \nholiday_fit = holiday |&gt;\n    model(\n      HWAdd = ETS(Trips ~ error(\"A\") + trend(\"A\") + \n                    season(\"A\", period = 4)),\n      HWMult = ETS(Trips ~ error(\"A\") + trend(\"A\") + \n                     season(\"M\", period = 4)))\n\n# Inspect fitted coefficients---note that gamma is really tiny in both models,\n# which means that the seasonal pattern doesn't change much over time\nholiday_fit |&gt; select(HWAdd) |&gt; coef()\n\n# A tibble: 9 × 3\n  .model term   estimate\n  &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;\n1 HWAdd  alpha  0.262   \n2 HWAdd  beta   0.0431  \n3 HWAdd  gamma  0.000100\n4 HWAdd  l[0]   9.79    \n5 HWAdd  b[0]   0.0211  \n6 HWAdd  s[0]  -0.534   \n7 HWAdd  s[-1] -0.670   \n8 HWAdd  s[-2] -0.294   \n9 HWAdd  s[-3]  1.50    \n\nholiday_fit |&gt; select(HWMult) |&gt; coef()\n\n# A tibble: 9 × 3\n  .model term   estimate\n  &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;\n1 HWMult alpha  0.223   \n2 HWMult beta   0.0311  \n3 HWMult gamma  0.000100\n4 HWMult l[0]   9.88    \n5 HWMult b[0]  -0.0279  \n6 HWMult s[0]   0.942   \n7 HWMult s[-1]  0.926   \n8 HWMult s[-2]  0.968   \n9 HWMult s[-3]  1.16    \n\n# An example of how we would pull out the components\nholiday_fit |&gt; select(HWAdd) |&gt; components() |&gt; head(10)\n\n# A dable: 10 x 7 [1Q]\n# Key:     .model [1]\n# :        Trips = lag(level, 1) + lag(slope, 1) + lag(season, 4) + remainder\n   .model Quarter Trips level     slope season remainder\n   &lt;chr&gt;    &lt;qtr&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 HWAdd  1997 Q1 NA    NA    NA         1.50    NA     \n 2 HWAdd  1997 Q2 NA    NA    NA        -0.294   NA     \n 3 HWAdd  1997 Q3 NA    NA    NA        -0.670   NA     \n 4 HWAdd  1997 Q4 NA     9.79  0.0211   -0.534   NA     \n 5 HWAdd  1998 Q1 11.8   9.94  0.0425    1.50     0.496 \n 6 HWAdd  1998 Q2  9.28  9.88  0.0245   -0.294   -0.415 \n 7 HWAdd  1998 Q3  8.64  9.75 -0.000841 -0.670   -0.588 \n 8 HWAdd  1998 Q4  9.30  9.77  0.00298  -0.534    0.0884\n 9 HWAdd  1999 Q1 11.2   9.75 -0.00124   1.50    -0.0976\n10 HWAdd  1999 Q2  9.61  9.79  0.00552  -0.294    0.157 \n\n# Plot the decomposition according to the components\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\ng1 = holiday_fit |&gt; select(HWAdd) |&gt; components() |&gt; autoplot() +\n  labs(title = \"Holt-Winters: additive seasonality\", subtitle = NULL) + \n  theme_bw()\ng2 = holiday_fit |&gt; select(HWMult) |&gt; components() |&gt; autoplot() +\n  labs(title = \"Holt-Winters: multiplicative seasonality\", subtitle = NULL) + \n  theme_bw()\ngrid.arrange(g1, g2, ncol = 2)  \n\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_line()`).\nRemoved 4 rows containing missing values or values outside the scale range\n(`geom_line()`)."
  },
  {
    "objectID": "lectures/spectral/spectral.html",
    "href": "lectures/spectral/spectral.html",
    "title": "Lecture 5: Spectral Analysis and Decomposition",
    "section": "",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(astsa)\nlibrary(fpp3)\nlibrary(epidatasets)\n\n\n\nPeriodic processes\n\nn = 100\nt = 1:n\nx1 = sqrt(2^2 + 3^2)*cos(2*pi*t*3/n + atan(-3/2))\ny1 = 2*cos(2*pi*t*3/n) + 3*sin(2*pi*t*3/n)\n\nx2 = sqrt(4^2 + 5^2)*cos(2*pi*t*6/n + atan(-5/4))\ny2 = 4*cos(2*pi*t*6/n) + 5*sin(2*pi*t*6/n)\n\npar(mar = c(4.5, 4.5, 0.5, 0.5))\nmatplot(t, cbind(x1, y1), type = \"l\", lty = 1:2, col = 1:2, xlab = \"Time\",\n        ylab = \"\", ylim = c(-10, 10))\nmatplot(t, cbind(x2, y2), type = \"l\", lty = 1:2, col = c(8,4), add = TRUE)\nrect(-atan(-3/2)*n/3*1/(2*pi), -20, n/3 - atan(-3/2)*n/3*1/(2*pi), 20,\n     border = NA, col = rgb(1, 0, 0, 0.2))\nrect(3*n/6 - atan(-5/4)*n/6*1/(2*pi), -20, 4*n/6 - atan(-6/4)*n/6*1/(2*pi), 20,\n     border = NA, col = rgb(0, 0, 1, 0.2))\n\n\n\n\n\n\n\n\n\n\nMixtures\n\npar(mfrow = c(2, 1), mar = c(4.5, 4.5, 0.5, 0.5))\nplot(t, y1 + y2, type = \"l\", xlab = \"Time\", ylab = \"\")\n\ny3 = 6*cos(2*pi*t*18/100) + 7*sin(2*pi*t*18/100)\nplot(t, y1 + y2 + y3, type = \"l\", xlab = \"Time\", ylab = \"\")\n\n\n\n\n\n\n\n\n\n\nPeriodogram, mixture data\n\nP = Mod(fft(y1 + y2))^2 / n\nQ = Mod(fft(y1 + y2 + y3))^2 / n\n\npar(mfrow = c(2, 1), mar = c(4.5, 4.5, 0.5, 0.5))\nplot(0:(n-1)/n, P, type = \"h\", xlab = \"Frequency\", ylab = \"Periodogram\", \n     xlim = c(0, 0.5), lwd = 3)\nabline(v = 0.5, lty = 2, col = 2)\nplot(0:(n-1)/n, Q, type = \"h\", xlab = \"Frequency\", ylab = \"Periodogram\",  \n     xlim = c(0, 0.5), lwd = 3)\nabline(v = 0.5, lty = 2, col = 2)\n\n\n\n\n\n\n\n\n\n\nPeriodogram, star data\n\nn = length(star)\nPer = Mod(fft(star - mean(star)))^2 / n\nFreq = 0:(n-1)/n\n\npar(mfrow = c(2, 1), mar = c(4.5, 4.5, 0.5, 0.5))\nplot(star, ylab=\"Star magnitude\", xlab = \"Day\")\nplot(Freq, Per, type = \"h\", xlab = \"Frequency\", ylab = \"Periodogram\",  \n     xlim = c(0, 0.08), lwd = 3)\ntext(0.05, 7000, \"24 day cycle\")\ntext(0.027, 9000, \"29 day cycle\")\n\n\n\n\n\n\n\n\n\n\nST decomposition\n\nus_retail_employment &lt;- us_employment |&gt;\n  filter(year(Month) &gt;= 1990, Title == \"Retail Trade\") |&gt;\n  select(-Series_ID, -Title) \n\nx = as.Date(us_retail_employment$Month)\ny = us_retail_employment$Employed\n\n# Compute and plot HP filter solution at decently large lambda value\nn = nrow(us_retail_employment)\nD = diag(rep(-2,n))       # -2s on the diagonal\nD[row(D) == col(D)-1] = 1 # 1s above the diagonal\nD[row(D) == col(D)+1] = 1 # 1s below the diagonal\nD = D[-c(1,n), ]          # Drop first and last row\nI = diag(n)               # n x n identity matrix\nlam = 1000\nhp = solve(I + lam * t(D) %*% D, y)\n\npar(mfrow = c(2, 1), mar = c(4.5, 4.5, 0.5, 0.5))\nplot(x, y, col = 8, type = \"l\", xlab = \"Date\", ylab = \"Employed\")\nlines(x, hp, type = \"l\", lty = 1, lwd = 2, col = 2)\n\n# Compute and plot periodogram and pick out 4 largest components (but ignoring\n# ones that bunched up together)\nd = fft(y - hp)\nPer = Mod(d)^2 / n\nFreq = 0:(n-1)/n\n\nplot(Freq, Per, type = \"h\", xlab = \"Frequency\",  ylab = \"Periodogram\", \n     xlim = c(0, 0.5), lwd = 2)\n\nord = order(Per[Freq &lt;= 1/2], decreasing = TRUE)\ncbind(Per[ord][1:5], Freq[ord][1:5])\n\n          [,1]       [,2]\n[1,] 3092160.9 0.08403361\n[2,] 1845802.0 0.16806723\n[3,] 1672142.4 0.16526611\n[4,]  921830.6 0.24929972\n[5,]  369734.6 0.33333333\n\nind = ord[c(1, 2, 4, 5)]\ntext(Freq[ind] + 0.005, Per[ind], adj = c(0, 0.5),\n     paste(round(1/Freq[ind], 2), \"month cycle\"))\n\n\n\n\n\n\n\n\n\n\nSpectral density, moving average\n\ntheta = seq(0, 0.9, length = 8)\nomega = seq(0, 1/2, length = 500)\nfmat = matrix(NA, length(omega), length(theta))\nfor (j in 1:ncol(fmat)) {\n  fmat[,j] = (1 + theta[j])^2 + 2* theta[j]^2 * cos(2*pi*omega)\n}\n\npar(mar = c(4.5, 4.5, 0.5, 0.5))\nmatplot(omega, fmat, type = \"l\", lty = 1, col = 1:8, \n        xlab = \"Frequency\", ylab = \"Spectral density\")\nlegend(\"topright\", lty = 1, col = 1:8,\n       legend = paste(\"theta =\", round(theta, 2)))\n\n\n\n\n\n\n\n\n\n\nSpectral density, autoregressive\n\nphi1 = c(1, 0.6, 0.5, -0.5)\nphi2 = c(-0.9, -0.9, 0.4, 0.4)\nomega = seq(0, 1/2, length = 500)\nfmat = matrix(NA, length(omega), length(theta))\nfor (j in 1:ncol(fmat)) {\n  fmat[,j] = 1 / ((1 + phi1[j]^2 + phi2[j]^2) -\n    2 * phi1[j] * (1 - phi2[j]) * cos(2*pi*omega) -\n    2 * phi2[j] * cos(4*pi*omega))\n}\n\npar(mar = c(4.5, 4.5, 0.5, 0.5))\nmatplot(omega, fmat, type = \"l\", lty = 1, col = 1:8, \n        xlab = \"Frequency\", ylab = \"Spectral density\")\nlegend(\"topright\", lty = 1, col = 1:8,\n       legend = paste0(\"(phi1, phi2) = (\", round(phi1, 2), \", \", \n                       round(phi2, 2), \")\"))"
  },
  {
    "objectID": "lectures/dependence/dependence.html",
    "href": "lectures/dependence/dependence.html",
    "title": "Lecture 2: Measures of Dependence and Stationarity",
    "section": "",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(astsa)\nlibrary(fpp3)\nlibrary(epidatasets)\n\n\n\nRandom walk\n\nn = 500 # number of time points\nr = 100 # number of repetitions\ndelta = 0.2 # drift\n\nx0 = matrix(rnorm(n * r), nrow = n)\ny0 = matrix(rnorm(n * r) + delta, nrow = n)\n\nx = apply(x0, 2, cumsum)\ny = apply(y0, 2, cumsum)\n\nmu_x = apply(x, 1, mean)\nmu_y = apply(y, 1, mean)\n\nmatplot(x, type = \"l\", lty = 1, col = rgb(0, 0, 0, 0.2),\n        xlab = \"t\", ylab = \"x_t\", main = \"delta = 0\")\nlines(mu_x, lty = 1, lwd = 3, col = 1)\n\n\n\n\n\n\n\nmatplot(y, type = \"l\", lty = 1, col = rgb(0.96, 0.28, 0.24, 0.2),\n        xlab = \"t\", ylab = \"x_t\", main = paste(\"delta =\", delta))\nlines(mu_y, lty = 1, lwd = 3, col = 2)\n\n\n\n\n\n\n\n\n\n\nAuto-correlation heatmaps\n\nn = 500\nm1 = m2 = matrix(0, n, n)\n\n# Moving avg autocorrelation matrix\nm1[row(m1) == col(m1)-2] = 1/3\nm1[row(m1) == col(m1)-1] = 2/3\nm1[row(m1) == col(m1)] = 1\nm1[row(m1) == col(m1)+1] = 2/3\nm1[row(m1) == col(m1)+2] = 1/3\n\n# Random walk autocorrelation matrix\nfor (s in 1:n) {\n  for (t in 1:n) {\n    m2[s,t] = min(s,t) / sqrt(s*t)\n  }\n}\n\n# Handy rotate function --- the orientation of R's image() function is to plot\n# a heamtap under a ***90 degrees counterclockwise rotation*** of the standard\n# way we think of matrices being laid out. So we can use this function below to\n# pre-rotate the matrix clockwise by 90 degrees, to effectively undo this and\n# have it print in a way that aligns with the standard matrix layout\nclockwise90 = function(a) { t(a[nrow(a):1,]) }\n\n# Now for the heatmaps\npar(mar = c(2,2,2,2), mfrow = c(1,2))\nimage(clockwise90(m1), main = \"Moving average\", xaxt = \"n\", yaxt = \"n\")\nimage(clockwise90(m2), main = \"Random walk\", xaxt = \"n\", yaxt = \"n\")\n\n\n\n\n\n\n\n\n\n\nCovid-19 cross-correlation\n\n# Covid-19 cases and deaths in California, pivot longer\ndf = cases_deaths_subset |&gt;\n  filter(geo_value == \"ca\") |&gt;\n  select(time_value, case_rate_7d_av, death_rate_7d_av) |&gt;\n  pivot_longer(cols = c(case_rate_7d_av, death_rate_7d_av)) |&gt;\n  mutate(name = recode(name, \n                       case_rate_7d_av = \"Cases\", \n                       death_rate_7d_av = \"Deaths\"))\n\n# Handy function to produce a transformation from one range to another\ntrans = function(x, from_range, to_range) {\n  (x - from_range[1]) / (from_range[2] - from_range[1]) *\n    (to_range[2] - to_range[1]) + to_range[1]\n}\n\n# Compute ranges of the two signals, and transformations in b/w them\nrange1 = df |&gt; filter(name == \"Cases\") |&gt; select(\"value\") |&gt; range()\nrange2 = df |&gt; filter(name == \"Deaths\") |&gt; select(\"value\") |&gt; range()\ntrans12 = function(x) trans(x, range1, range2)\ntrans21 = function(x) trans(x, range2, range1)\n\nggplot(bind_rows(\n  df |&gt; filter(name == \"Cases\"),\n  df |&gt; filter(name == \"Deaths\") |&gt; mutate_at(\"value\", trans21)),\n  aes(x = time_value, y = value)) +\n  geom_line(aes(color = name)) +\n  scale_color_manual(values = palette()[c(2,4)]) +\n  scale_y_continuous(\n    name = \"Reported Covid-19 cases per 100k people\", \n    limits = range1,\n    sec.axis = sec_axis(\n      trans = trans12, \n      name = \"Reported Covid-19 deaths per 100k people\")) +\n  labs(title = \"Covid-19 cases and deaths in California\", x = \"Date\") +\n  theme_bw() + \n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\nWarning: The `trans` argument of `sec_axis()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\n\n\n\n\n\n\n\nccf(df |&gt; filter(name == \"Cases\") |&gt; select(value),\n    df |&gt; filter(name == \"Deaths\") |&gt; select(value),\n    lag.max = 40, ylab = \"Cross-correlation\", main = \"\")\n\n\n\n\n\n\n\n\n\n\nSpeech auto-correlation\n\nplot(speech, type = \"l\", ylab = \"Vocal response\")\n\n\n\n\n\n\n\nacf(speech, lag.max = 250, ylab = \"Auto-correlation\", main = \"\")"
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "Homework",
    "section": "",
    "text": "Homework 1\nRmd, html, source\n\n\nHomework 2\nRmd, html, source\n\n\nHomework 3\nRmd, html, source\n\n\nHomework 4\nRmd, html, source\n\n\n\nHomework 5 has been canceled; see bCourses for new Homework 5."
  }
]